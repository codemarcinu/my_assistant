"""
Real LLM End-to-End Tests

Testy z rzeczywistymi modelami Ollama LLM - bez mock√≥w:
- Realne odpowiedzi AI z modeli mistral:7b, llama3.2:3b i bielik-1.5b-v3.0-instruct
- Testowanie jako≈õci odpowiedzi
- Pomiar wydajno≈õci i czas√≥w odpowiedzi
- Testowanie r√≥≈ºnych typ√≥w zapyta≈Ñ
- Sprawdzanie kontekstu i pamiƒôci
"""

import pytest
import asyncio
import json
import os
import time
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

import httpx
from fastapi.testclient import TestClient

from backend.models.shopping import Product, ShoppingTrip
from backend.models.rag_document import RAGDocument
from datetime import date


class TestRealLLME2E:
    """Testy E2E z rzeczywistymi modelami LLM"""
    
    @pytest.fixture(autouse=True)
    def setup_ollama_client(self):
        """Konfiguracja klienta Ollama dla test√≥w"""
        import ollama
        from backend.core.llm_client import ollama_client
        
        # Zaktualizuj klienta Ollama dla test√≥w
        test_client = ollama.Client(host="http://localhost:11434")
        
        # ZastƒÖp globalny klient testowym
        import backend.core.llm_client
        backend.core.llm_client.ollama_client = test_client
        
        yield
        
        # Przywr√≥ƒá oryginalny klient po te≈õcie
        backend.core.llm_client.ollama_client = ollama.Client(host="http://ollama:11434")
    
    @pytest.fixture
    def test_client(self):
        """Klient testowy FastAPI"""
        from backend.main import app
        return TestClient(app)
    
    @pytest.fixture
    def db_session(self):
        """Sesja bazy danych"""
        from backend.tests.conftest import db_session
        return db_session
    
    @pytest.fixture
    def sample_food_data(self):
        """Przyk≈Çadowe dane produkt√≥w spo≈ºywczych"""
        return [
            {
                "name": "Mleko 3.2%",
                "quantity": 2,
                "unit": "l",
                "expiry_date": date(2024, 12, 31),
                "category": "nabia≈Ç"
            },
            {
                "name": "Chleb razowy",
                "quantity": 1,
                "unit": "szt",
                "expiry_date": date(2024, 12, 25),
                "category": "pieczywo"
            },
            {
                "name": "Jajka",
                "quantity": 10,
                "unit": "szt",
                "expiry_date": date(2024, 12, 28),
                "category": "nabia≈Ç"
            },
            {
                "name": "Pomidory",
                "quantity": 500,
                "unit": "g",
                "expiry_date": date(2024, 12, 20),
                "category": "warzywa"
            },
            {
                "name": "Kurczak piersi",
                "quantity": 1,
                "unit": "kg",
                "expiry_date": date(2024, 12, 22),
                "category": "miƒôso"
            }
        ]
    
    @pytest.fixture
    def real_receipt_files(self):
        """Prawdziwe pliki paragon√≥w do test√≥w"""
        receipts_dir = Path("../../receipts")
        if receipts_dir.exists():
            return list(receipts_dir.glob("*"))
        return []
    
    def test_ollama_models_availability(self):
        """Test dostƒôpno≈õci modeli Ollama"""
        try:
            import httpx
            response = httpx.get("http://localhost:11434/api/tags")
            assert response.status_code == 200
            
            models = response.json()
            model_names = [model["name"] for model in models.get("models", [])]
            
            print(f"Dostƒôpne modele: {model_names}")
            
            # Sprawd≈∫ czy mamy wymagane modele
            assert "mistral:7b" in model_names, "Model mistral:7b nie jest dostƒôpny"
            assert "llama3.2:3b" in model_names, "Model llama3.2:3b nie jest dostƒôpny"
            assert "SpeakLeash/bielik-1.5b-v3.0-instruct:FP16" in model_names, "Model bielik nie jest dostƒôpny"
            
            print("‚úÖ Wszystkie wymagane modele sƒÖ dostƒôpne")
            
        except Exception as e:
            pytest.fail(f"B≈ÇƒÖd po≈ÇƒÖczenia z Ollama: {e}")
    
    @pytest.mark.asyncio
    async def test_real_llm_food_questions(self, test_client):
        """Test rzeczywistych odpowiedzi AI na pytania o jedzenie"""
        food_questions = [
            "Jakie produkty sƒÖ dobre na ≈õniadanie?",
            "Co mogƒô ugotowaƒá z kurczakiem?",
            "Jakie warzywa sƒÖ sezonowe w grudniu?",
            "Czy mleko jest zdrowe?",
            "Jakie produkty majƒÖ du≈ºo bia≈Çka?"
        ]
        
        responses = []
        
        for i, question in enumerate(food_questions, 1):
            print(f"\n--- Pytanie {i}: {question} ---")
            
            start_time = time.time()
            
            # Debug: sprawd≈∫ typ obiektu zwracanego przez LLM client
            from backend.core.llm_client import llm_client
            import inspect
            
            try:
                # Test bezpo≈õrednio LLM client
                print(f"Testing LLM client directly...")
                
                # Sprawd≈∫ czy to ten sam obiekt
                print(f"LLM client object: {llm_client}")
                print(f"LLM client type: {type(llm_client)}")
                
                # Test _stream_response bezpo≈õrednio
                print(f"Testing _stream_response directly...")
                messages = [{"role": "user", "content": question}]
                sync_gen = llm_client._stream_response("mistral:7b", messages, {})
                print(f"_stream_response type: {type(sync_gen)}")
                print(f"Is generator: {inspect.isgenerator(sync_gen)}")
                
                # Test pierwszy chunk
                try:
                    first_chunk = next(sync_gen)
                    print(f"First chunk from _stream_response: {first_chunk}")
                except Exception as e:
                    print(f"Error getting first chunk: {e}")
                
                # Test dok≈Çadnie tak samo jak w generate_stream_from_prompt_async
                print(f"Testing exact same call as generate_stream_from_prompt_async...")
                messages_async = []
                messages_async.append({"role": "user", "content": question})
                sync_gen_async = llm_client._stream_response("mistral:7b", messages_async, {})
                print(f"_stream_response (async style) type: {type(sync_gen_async)}")
                print(f"Is generator (async style): {inspect.isgenerator(sync_gen_async)}")
                
                # Test generate_stream_from_prompt_async bezpo≈õrednio
                print(f"Testing generate_stream_from_prompt_async directly...")
                async_gen = llm_client.generate_stream_from_prompt_async(
                    model="mistral:7b", 
                    prompt=question
                )
                print(f"Async generator type: {type(async_gen)}")
                print(f"Is async generator: {inspect.isasyncgen(async_gen)}")
                
                # Przetestuj pierwszy chunk
                first_chunk = await anext(async_gen)
                print(f"First chunk: {first_chunk}")
                
                # Je≈õli to dzia≈Ça, u≈ºyj normalnego endpointu
                response = test_client.post(
                    "/api/chat/chat",
                    json={"prompt": question}
                )
            except Exception as e:
                print(f"Error testing LLM client: {e}")
                response = test_client.post(
                    "/api/chat/chat",
                    json={"prompt": question}
                )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            assert response.status_code == 200, f"B≈ÇƒÖd HTTP: {response.status_code}"
            
            response_data = response.json()
            print(f"Odpowied≈∫: {response_data.get('data', 'Brak odpowiedzi')[:200]}...")
            print(f"Czas odpowiedzi: {response_time:.2f}s")
            
            # Sprawd≈∫ jako≈õƒá odpowiedzi
            assert "data" in response_data, "Brak pola 'data' w odpowiedzi"
            assert len(response_data["data"]) > 10, "Odpowied≈∫ jest za kr√≥tka"
            
            responses.append({
                "question": question,
                "response": response_data["data"],
                "response_time": response_time
            })
        
        # Analiza wynik√≥w
        avg_response_time = sum(r["response_time"] for r in responses) / len(responses)
        print(f"\nüìä ≈öredni czas odpowiedzi: {avg_response_time:.2f}s")
        
        # Sprawd≈∫ czy odpowiedzi sƒÖ sensowne
        for resp in responses:
            assert len(resp["response"]) > 20, f"Odpowied≈∫ za kr√≥tka: {resp['response']}"
            assert resp["response_time"] < 30, f"Odpowied≈∫ za wolna: {resp['response_time']}s"
    
    @pytest.mark.asyncio
    async def test_real_llm_meal_planning(self, test_client):
        """Test rzeczywistego planowania posi≈Çk√≥w przez AI"""
        meal_planning_questions = [
            "Zaplanuj posi≈Çki na poniedzia≈Çek",
            "Co ugotowaƒá na obiad z produkt√≥w: kurczak, pomidory, makaron?",
            "Przepis na szybkie ≈õniadanie",
            "Plan posi≈Çk√≥w na ca≈Çy tydzie≈Ñ",
            "Co zrobiƒá z resztkami jedzenia?"
        ]
        
        for i, question in enumerate(meal_planning_questions, 1):
            print(f"\n--- Planowanie posi≈Çk√≥w {i}: {question} ---")
            
            start_time = time.time()
            
            response = test_client.post(
                "/api/chat/chat",
                json={"prompt": question}
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            assert response.status_code == 200
            response_data = response.json()
            
            print(f"Plan: {response_data.get('data', '')[:300]}...")
            print(f"Czas: {response_time:.2f}s")
            
            # Sprawd≈∫ czy odpowied≈∫ zawiera elementy planowania
            response_text = response_data.get("data", "").lower()
            planning_keywords = ["plan", "przepis", "posi≈Çek", "gotowaƒá", "ugotowaƒá", "menu"]
            has_planning_content = any(keyword in response_text for keyword in planning_keywords)
            
            assert has_planning_content, f"Odpowied≈∫ nie zawiera element√≥w planowania: {response_text[:100]}"
            assert response_time < 45, f"Planowanie za wolne: {response_time}s"
    
    @pytest.mark.asyncio
    async def test_real_llm_weather_queries(self, test_client):
        """Test zapyta≈Ñ o pogodƒô z realnym AI"""
        weather_questions = [
            "Jaka jest pogoda w Warszawie?",
            "Czy bƒôdzie padaƒá jutro?",
            "Temperatura na weekend",
            "Pogoda na wakacje"
        ]
        
        for question in weather_questions:
            print(f"\n--- Pogoda: {question} ---")
            
            start_time = time.time()
            
            response = test_client.post(
                "/api/chat/chat",
                json={"prompt": question}
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            assert response.status_code == 200
            response_data = response.json()
            
            print(f"Odpowied≈∫: {response_data.get('data', '')[:200]}...")
            print(f"Czas: {response_time:.2f}s")
            
            # Sprawd≈∫ czy AI pr√≥buje odpowiedzieƒá na pytanie o pogodƒô
            response_text = response_data.get("data", "").lower()
            weather_keywords = ["pogoda", "temperatura", "deszcz", "s≈Ço≈Ñce", "stopnie"]
            has_weather_content = any(keyword in response_text for keyword in weather_keywords)
            
            assert has_weather_content or "nie mogƒô" in response_text, f"AI nie odpowiedzia≈Ç na pytanie o pogodƒô"
    
    @pytest.mark.asyncio
    async def test_real_llm_conversation_context(self, test_client):
        """Test kontekstu konwersacji z realnym AI"""
        conversation = [
            "Nazywam siƒô Jan Kowalski",
            "Jakie jest moje imiƒô?",
            "Lubiƒô gotowaƒá w≈Çoskie dania",
            "Co mogƒô ugotowaƒá zgodnie z moimi preferencjami?",
            "Przypomnij mi moje imiƒô"
        ]
        
        conversation_id = None
        
        for i, message in enumerate(conversation, 1):
            print(f"\n--- Wiadomo≈õƒá {i}: {message} ---")
            
            request_data = {"prompt": message}
            if conversation_id:
                request_data["conversation_id"] = conversation_id
            
            response = test_client.post(
                "/api/chat/chat",
                json=request_data
            )
            
            assert response.status_code == 200
            response_data = response.json()
            
            # Zapisz conversation_id z pierwszej odpowiedzi
            if i == 1 and "conversation_id" in response_data:
                conversation_id = response_data["conversation_id"]
            
            print(f"Odpowied≈∫: {response_data.get('data', '')[:150]}...")
            
            # Sprawd≈∫ czy AI pamiƒôta kontekst
            if i == 2:  # "Jakie jest moje imiƒô?"
                response_text = response_data.get("data", "").lower()
                assert "jan" in response_text or "kowalski" in response_text, "AI nie pamiƒôta imienia"
            
            elif i == 4:  # "Co mogƒô ugotowaƒá zgodnie z moimi preferencjami?"
                response_text = response_data.get("data", "").lower()
                italian_keywords = ["w≈Çoskie", "pasta", "pizza", "spaghetti", "w≈Çochy"]
                has_italian_content = any(keyword in response_text for keyword in italian_keywords)
                assert has_italian_content, "AI nie pamiƒôta preferencji kulinarnych"
    
    @pytest.mark.asyncio
    async def test_real_llm_performance_benchmark(self, test_client):
        """Benchmark wydajno≈õci realnego AI"""
        test_prompts = [
            "Kr√≥tkie pytanie",
            "D≈Çu≈ºsze pytanie o gotowanie i przepisy kulinarne",
            "Bardzo szczeg√≥≈Çowe pytanie o planowanie posi≈Çk√≥w na ca≈Çy tydzie≈Ñ z uwzglƒôdnieniem diety wegetaria≈Ñskiej"
        ]
        
        performance_results = []
        
        for prompt in test_prompts:
            print(f"\n--- Benchmark: {prompt[:30]}... ---")
            
            # Wykonaj 3 testy dla ka≈ºdego promptu
            times = []
            for _ in range(3):
                start_time = time.time()
                
                response = test_client.post(
                    "/api/chat/chat",
                    json={"prompt": prompt}
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                times.append(response_time)
                
                assert response.status_code == 200
                
                # Kr√≥tka przerwa miƒôdzy testami
                await asyncio.sleep(1)
            
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            performance_results.append({
                "prompt_length": len(prompt),
                "avg_time": avg_time,
                "min_time": min_time,
                "max_time": max_time
            })
            
            print(f"≈öredni czas: {avg_time:.2f}s (min: {min_time:.2f}s, max: {max_time:.2f}s)")
        
        # Analiza wynik√≥w
        print(f"\nüìä WYNIKI BENCHMARKU:")
        for i, result in enumerate(performance_results, 1):
            print(f"Test {i}: {result['prompt_length']} znak√≥w - {result['avg_time']:.2f}s")
        
        # Sprawd≈∫ czy wydajno≈õƒá jest akceptowalna
        for result in performance_results:
            assert result["avg_time"] < 30, f"Za wolna odpowied≈∫: {result['avg_time']}s"
            assert result["max_time"] < 45, f"Za wolna maksymalna odpowied≈∫: {result['max_time']}s"
    
    @pytest.mark.asyncio
    async def test_real_llm_error_handling(self, test_client):
        """Test obs≈Çugi b≈Çƒôd√≥w z realnym AI"""
        problematic_prompts = [
            "",  # Pusty prompt
            "a" * 10000,  # Bardzo d≈Çugi prompt
            "ü§ñüöÄüéâ" * 100,  # Emoji spam
            "SELECT * FROM users; DROP TABLE users;",  # SQL injection attempt
        ]
        
        for prompt in problematic_prompts:
            print(f"\n--- Test b≈Çƒôdu: {prompt[:50]}... ---")
            
            response = test_client.post(
                "/api/chat/chat",
                json={"prompt": prompt}
            )
            
            # System powinien obs≈Çu≈ºyƒá b≈Çƒôdy gracefully
            if response.status_code == 200:
                response_data = response.json()
                print(f"Odpowied≈∫: {response_data.get('data', '')[:100]}...")
                
                # Sprawd≈∫ czy AI pr√≥buje odpowiedzieƒá sensownie
                response_text = response_data.get("data", "")
                assert len(response_text) > 0, "Pusta odpowied≈∫ na problematyczny prompt"
            else:
                print(f"B≈ÇƒÖd HTTP: {response.status_code}")
                # Sprawd≈∫ czy b≈ÇƒÖd jest obs≈Çu≈ºony gracefully
                assert response.status_code in [400, 422], f"Nieoczekiwany kod b≈Çƒôdu: {response.status_code}"
    
    @pytest.mark.asyncio
    async def test_real_llm_rag_integration(self, test_client, db_session):
        """Test integracji RAG z realnym AI"""
        session = await anext(db_session)
        try:
            # Dodaj dokumenty do bazy RAG
            rag_documents = [
                {
                    "content": "Przepis na spaghetti carbonara: makaron, jajka, boczek, parmezan, pieprz. Gotuj makaron, podsma≈º boczek, wymieszaj z jajkami i serem.",
                    "doc_metadata": {"type": "recipe", "cuisine": "w≈Çoska", "difficulty": "≈Çatwe"}
                },
                {
                    "content": "W≈Ça≈õciwo≈õci zdrowotne broku≈Ç√≥w: witamina C, b≈Çonnik, przeciwutleniacze. Broku≈Çy sƒÖ bogate w sulforafan, kt√≥ry ma w≈Ça≈õciwo≈õci przeciwnowotworowe.",
                    "doc_metadata": {"type": "nutrition", "category": "warzywa", "benefits": "zdrowie"}
                },
                {
                    "content": "Przechowywanie ≈ºywno≈õci: mleko w lod√≥wce 3-5 dni, jajka 3-5 tygodni, miƒôso mielone 1-2 dni, warzywa 1-2 tygodnie.",
                    "doc_metadata": {"type": "storage", "category": "porady", "importance": "wysoka"}
                }
            ]
            
            for doc_data in rag_documents:
                rag_doc = RAGDocument(
                    content=doc_data["content"],
                    doc_metadata=doc_data["doc_metadata"]
                )
                session.add(rag_doc)
            await session.commit()
            
            # Test zapyta≈Ñ RAG
            rag_questions = [
                "Jak ugotowaƒá spaghetti carbonara?",
                "Jakie sƒÖ w≈Ça≈õciwo≈õci zdrowotne broku≈Ç√≥w?",
                "Jak d≈Çugo mo≈ºna przechowywaƒá mleko?",
                "Co to jest sulforafan?"
            ]
            
            for question in rag_questions:
                print(f"\n--- RAG: {question} ---")
                
                start_time = time.time()
                
                response = test_client.post(
                    "/api/chat/chat",
                    json={"prompt": question}
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                
                assert response.status_code == 200
                response_data = response.json()
                
                print(f"Odpowied≈∫: {response_data.get('data', '')[:200]}...")
                print(f"Czas: {response_time:.2f}s")
                
                # Sprawd≈∫ czy odpowied≈∫ zawiera informacje z dokument√≥w RAG
                response_text = response_data.get("data", "").lower()
                
                if "carbonara" in question.lower():
                    carbonara_keywords = ["makaron", "jajka", "boczek", "parmezan", "w≈Çoska"]
                    has_carbonara_content = any(keyword in response_text for keyword in carbonara_keywords)
                    assert has_carbonara_content, "AI nie u≈ºy≈Ç informacji o carbonara z RAG"
                
                elif "broku≈Çy" in question.lower():
                    broccoli_keywords = ["witamina", "b≈Çonnik", "przeciwutleniacze", "sulforafan"]
                    has_broccoli_content = any(keyword in response_text for keyword in broccoli_keywords)
                    assert has_broccoli_content, "AI nie u≈ºy≈Ç informacji o broku≈Çach z RAG"
                
                elif "mleko" in question.lower():
                    milk_keywords = ["lod√≥wka", "dni", "przechowywanie"]
                    has_milk_content = any(keyword in response_text for keyword in milk_keywords)
                    assert has_milk_content, "AI nie u≈ºy≈Ç informacji o przechowywaniu z RAG"
        
        finally:
            await session.close()
    
    @pytest.mark.asyncio
    async def test_real_llm_complex_scenarios(self, test_client):
        """Test z≈Ço≈ºonych scenariuszy z realnym AI"""
        complex_scenarios = [
            {
                "name": "Planowanie tygodniowego menu",
                "prompt": "Zaplanuj menu na ca≈Çy tydzie≈Ñ dla 4-osobowej rodziny z uwzglƒôdnieniem bud≈ºetu 500 z≈Ç, preferencji wegetaria≈Ñskich i alergii na orzechy. Uwzglƒôdnij ≈õniadania, obiady i kolacje."
            },
            {
                "name": "Analiza paragonu i porady",
                "prompt": "Kupi≈Çem: mleko 3.2% 2l, chleb razowy 1szt, jajka 10szt, pomidory 500g, kurczak piersi 1kg. Zaplanuj posi≈Çki na 3 dni i podaj porady dotyczƒÖce przechowywania."
            },
            {
                "name": "Dieta i zdrowie",
                "prompt": "Jestem na diecie redukcyjnej, ƒáwiczƒô 3x w tygodniu, mam 30 lat. Zaplanuj jad≈Çospis na 7 dni z kaloryczno≈õciƒÖ 1800 kcal dziennie, bogaty w bia≈Çko."
            }
        ]
        
        for scenario in complex_scenarios:
            print(f"\n--- {scenario['name']} ---")
            print(f"Prompt: {scenario['prompt'][:100]}...")
            
            start_time = time.time()
            
            response = test_client.post(
                "/api/chat/chat",
                json={"prompt": scenario["prompt"]}
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            assert response.status_code == 200
            response_data = response.json()
            
            response_text = response_data.get("data", "")
            print(f"Odpowied≈∫: {response_text[:300]}...")
            print(f"Czas: {response_time:.2f}s")
            print(f"D≈Çugo≈õƒá odpowiedzi: {len(response_text)} znak√≥w")
            
            # Sprawd≈∫ jako≈õƒá odpowiedzi
            assert len(response_text) > 100, f"Odpowied≈∫ za kr√≥tka dla z≈Ço≈ºonego scenariusza"
            assert response_time < 60, f"Odpowied≈∫ za wolna: {response_time}s"
            
            # Sprawd≈∫ czy AI pr√≥buje odpowiedzieƒá na wszystkie aspekty pytania
            if "menu" in scenario["name"].lower():
                menu_keywords = ["poniedzia≈Çek", "wtorek", "≈õroda", "menu", "plan"]
                has_menu_content = any(keyword in response_text.lower() for keyword in menu_keywords)
                assert has_menu_content, "AI nie zaplanowa≈Ç menu"
            
            elif "paragon" in scenario["name"].lower():
                receipt_keywords = ["mleko", "chleb", "jajka", "przechowywanie", "posi≈Çek"]
                has_receipt_content = any(keyword in response_text.lower() for keyword in receipt_keywords)
                assert has_receipt_content, "AI nie przeanalizowa≈Ç paragonu"
            
            elif "dieta" in scenario["name"].lower():
                diet_keywords = ["kalorie", "bia≈Çko", "dieta", "1800", "redukcyjna"]
                has_diet_content = any(keyword in response_text.lower() for keyword in diet_keywords)
                assert has_diet_content, "AI nie uwzglƒôdni≈Ç wymaga≈Ñ diety"
    
    def test_real_llm_system_health_under_load(self, test_client):
        """Test zdrowia systemu pod obciƒÖ≈ºeniem"""
        print("\n--- Test obciƒÖ≈ºenia systemu ---")
        
        # Wykonaj 10 r√≥wnoczesnych zapyta≈Ñ
        import threading
        import queue
        
        results = queue.Queue()
        
        def make_request(request_id):
            try:
                start_time = time.time()
                response = test_client.post(
                    "/api/chat/chat",
                    json={"prompt": f"Kr√≥tkie pytanie testowe #{request_id}"}
                )
                end_time = time.time()
                
                results.put({
                    "id": request_id,
                    "status_code": response.status_code,
                    "response_time": end_time - start_time,
                    "success": response.status_code == 200
                })
            except Exception as e:
                results.put({
                    "id": request_id,
                    "error": str(e),
                    "success": False
                })
        
        # Uruchom 10 wƒÖtk√≥w
        threads = []
        for i in range(10):
            thread = threading.Thread(target=make_request, args=(i,))
            threads.append(thread)
            thread.start()
        
        # Czekaj na zako≈Ñczenie wszystkich wƒÖtk√≥w
        for thread in threads:
            thread.join()
        
        # Zbierz wyniki
        test_results = []
        while not results.empty():
            test_results.append(results.get())
        
        # Analiza wynik√≥w
        successful_requests = [r for r in test_results if r["success"]]
        failed_requests = [r for r in test_results if not r["success"]]
        
        print(f"‚úÖ Udane zapytania: {len(successful_requests)}/10")
        print(f"‚ùå Nieudane zapytania: {len(failed_requests)}/10")
        
        if successful_requests:
            avg_response_time = sum(r["response_time"] for r in successful_requests) / len(successful_requests)
            max_response_time = max(r["response_time"] for r in successful_requests)
            min_response_time = min(r["response_time"] for r in successful_requests)
            
            print(f"üìä ≈öredni czas odpowiedzi: {avg_response_time:.2f}s")
            print(f"üìä Min czas: {min_response_time:.2f}s, Max czas: {max_response_time:.2f}s")
        
        # Sprawd≈∫ czy system radzi sobie z obciƒÖ≈ºeniem
        success_rate = len(successful_requests) / len(test_results)
        assert success_rate >= 0.8, f"Za niski wsp√≥≈Çczynnik sukcesu: {success_rate:.2%}"
        
        if successful_requests:
            avg_time = sum(r["response_time"] for r in successful_requests) / len(successful_requests)
            assert avg_time < 30, f"Za wolna ≈õrednia odpowied≈∫ pod obciƒÖ≈ºeniem: {avg_time:.2f}s"
    
    def test_real_llm_final_integration(self, test_client):
        """Finalny test integracji wszystkich komponent√≥w z realnym AI"""
        print("\n--- FINALNY TEST INTEGRACJI ---")
        
        # Kompleksowy scenariusz u≈ºytkownika
        user_scenario = [
            "Cze≈õƒá! Nazywam siƒô Anna i szukam pomocy w zarzƒÖdzaniu ≈ºywno≈õciƒÖ.",
            "Kupi≈Çem dzisiaj: mleko 2l, chleb, jajka 10szt, pomidory 500g, kurczak 1kg.",
            "Jak d≈Çugo mogƒô przechowywaƒá te produkty?",
            "Zaplanuj posi≈Çki na 3 dni z tych produkt√≥w.",
            "Czy te produkty sƒÖ zdrowe?",
            "Dziƒôkujƒô za pomoc!"
        ]
        
        conversation_id = None
        total_response_time = 0
        responses = []
        
        for i, message in enumerate(user_scenario, 1):
            print(f"\n--- Krok {i}: {message} ---")
            
            request_data = {"prompt": message}
            if conversation_id:
                request_data["conversation_id"] = conversation_id
            
            start_time = time.time()
            
            response = test_client.post(
                "/api/chat/chat",
                json=request_data
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            total_response_time += response_time
            
            assert response.status_code == 200, f"B≈ÇƒÖd w kroku {i}: {response.status_code}"
            
            response_data = response.json()
            
            # Zapisz conversation_id
            if i == 1 and "conversation_id" in response_data:
                conversation_id = response_data["conversation_id"]
            
            response_text = response_data.get("data", "")
            responses.append({
                "step": i,
                "user_message": message,
                "ai_response": response_text,
                "response_time": response_time
            })
            
            print(f"AI: {response_text[:150]}...")
            print(f"Czas: {response_time:.2f}s")
        
        # Analiza ko≈Ñcowa
        print(f"\nüìä PODSUMOWANIE INTEGRACJI:")
        print(f"≈ÅƒÖczny czas: {total_response_time:.2f}s")
        print(f"≈öredni czas na krok: {total_response_time/len(user_scenario):.2f}s")
        print(f"Liczba krok√≥w: {len(user_scenario)}")
        
        # Sprawd≈∫ jako≈õƒá ca≈Çej konwersacji
        assert total_response_time < 300, f"Ca≈Çkowity czas za d≈Çugi: {total_response_time}s"
        
        # Sprawd≈∫ czy AI pamiƒôta kontekst przez ca≈ÇƒÖ konwersacjƒô
        final_response = responses[-1]["ai_response"].lower()
        context_keywords = ["anna", "mleko", "chleb", "jajka", "pomidory", "kurczak"]
        has_context = any(keyword in final_response for keyword in context_keywords)
        
        assert has_context, "AI straci≈Ç kontekst w ko≈Ñcowej odpowiedzi"
        
        print("‚úÖ FINALNY TEST INTEGRACJI ZAKO≈ÉCZONY SUKCESEM!")
        print("üéâ System AI dzia≈Ça poprawnie z rzeczywistymi modelami LLM!")


if __name__ == "__main__":
    # Uruchom testy z realnymi modelami
    pytest.main([__file__, "-v", "-s"]) 