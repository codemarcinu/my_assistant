# FoodSave AI - Backend (FastAPI)

Nowoczesny backend dla inteligentnego asystenta spiÅ¼arni, oparty o FastAPI, Python 3.12, SQLAlchemy, Pydantic, Prometheus, OpenTelemetry, z peÅ‚nÄ… obsÅ‚ugÄ… async, testÃ³w i healthcheckÃ³w.

## ğŸš€ Szybki Start

### 1. Wymagania
- Python 3.12+
- Poetry (zalecane)
- Docker (jeÅ›li chcesz uruchomiÄ‡ caÅ‚oÅ›Ä‡ przez Compose)

### 2. Instalacja zaleÅ¼noÅ›ci
```bash
cd src/backend
poetry install
```

### 3. Uruchomienie backendu
```bash
poetry run uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 4. Testy
```bash
poetry run pytest
poetry run pytest --cov=src --cov-report=html
```

## ğŸ§ª Testowanie
- Wszystkie testy async muszÄ… mieÄ‡ dekorator `@pytest.mark.asyncio`.
- Testy jednostkowe i integracyjne w `src/backend/tests/`.
- Pokrycie kodu: `pytest --cov=src --cov-report=html`

## ğŸ—ï¸ Struktura katalogÃ³w
```
src/backend/
â”œâ”€â”€ main.py                # FastAPI app instance
â”œâ”€â”€ app_factory.py         # Tworzenie instancji FastAPI
â”œâ”€â”€ api/                   # Endpointy (routery)
â”œâ”€â”€ agents/                # Klasy agentÃ³w, orchestratory
â”œâ”€â”€ core/                  # Baza, cache, monitoring, utils
â”œâ”€â”€ models/                # SQLAlchemy + Pydantic
â”œâ”€â”€ services/              # Logika domenowa
â”œâ”€â”€ tests/                 # Testy unit/integration
â”œâ”€â”€ requirements.txt       # Dodatkowe zaleÅ¼noÅ›ci
â”œâ”€â”€ Dockerfile             # Docker backendu
â””â”€â”€ ...
```

## ğŸ³ Docker Compose
- Backend uruchamiany przez `docker-compose.yaml` (serwis: backend)
- Port domyÅ›lny: 8000
- Healthcheck: `/health`

## ğŸ”„ Monitoring i Health
- `/health` â€” ogÃ³lny health check (200 OK)
- `/ready` â€” gotowoÅ›Ä‡ do obsÅ‚ugi Å¼Ä…daÅ„
- `/metrics` â€” Prometheus metrics

## ğŸ” BezpieczeÅ„stwo
- Zmienna Å›rodowiskowa `SECRET_KEY` (w .env)
- Brak kluczy w kodzie ÅºrÃ³dÅ‚owym
- CORS: domyÅ›lnie tylko `http://localhost:3000`
- Wymagane klucze API: `OPENWEATHER_API_KEY`, `PERPLEXITY_API_KEY` (opcjonalne)

## ğŸ”‘ Konfiguracja API Keys
Dla peÅ‚nej funkcjonalnoÅ›ci aplikacji, ustaw nastÄ™pujÄ…ce klucze w pliku `.env`:

```bash
# Wymagane dla pogody
OPENWEATHER_API_KEY=your_openweather_api_key_here

# Opcjonalne dla zaawansowanych funkcji
PERPLEXITY_API_KEY=your_perplexity_api_key_here
```

**Uwaga**: Aplikacja dziaÅ‚a rÃ³wnieÅ¼ bez kluczy API - funkcje wymagajÄ…ce kluczy bÄ™dÄ… wyÅ‚Ä…czone automatycznie.

## ğŸ§© NajwaÅ¼niejsze zaleÅ¼noÅ›ci
- FastAPI, Uvicorn, SQLAlchemy, Pydantic, Prometheus, OpenTelemetry, Redis, pytest, slowapi (rate limiting)

## ğŸ§° Troubleshooting
- SprawdÅº logi: `docker compose logs -f backend`
- SprawdÅº health: `curl http://localhost:8000/health`
- SprawdÅº migracje DB: `alembic upgrade head`
- SprawdÅº konfiguracjÄ™: `.env` i `config.py`

## ğŸ“„ Licencja
MIT License - zobacz [LICENSE](LICENSE) dla szczegÃ³Å‚Ã³w.

---

ZgodnoÅ›Ä‡ z `.cursorrules` i najlepszymi praktykami FastAPI.

## ğŸ”„ Streaming Responses

Backend obsÅ‚uguje streaming responses dla czatu AI z nastÄ™pujÄ…cymi endpointami:
- `/api/chat` - Podstawowy endpoint czatu ze streamingiem
- `/api/v2/chat` - Rozszerzony endpoint czatu z dodatkowymi funkcjami

### Implementacja streaming
- Asynchroniczne generatory dla kompatybilnoÅ›ci z FastAPI
- Konwersja synchronicznych generatorÃ³w LLM na asynchroniczne
- Robust pattern z threading i queue dla stabilnoÅ›ci
- ObsÅ‚uga bÅ‚Ä™dÃ³w i graceful degradation

### PrzykÅ‚ad uÅ¼ycia
```python
# Streaming response w FastAPI
async def chat_stream_generator():
    async for chunk in llm_client.stream_chat(messages):
        yield f"data: {chunk}\n\n"
```

## Testowanie endpointÃ³w API v2

Aby przetestowaÄ‡ streamingowy endpoint `/api/v2/chat/stream`, dodaj test integracyjny w pliku `src/backend/tests/test_chat_endpoint.py`:

```python
from fastapi.testclient import TestClient
from fastapi import status
from src.backend.main import app

def test_chat_stream_basic():
    client = TestClient(app)
    payload = {"message": "Hello, how are you?", "session_id": "test-session"}
    response = client.post("/api/v2/chat/stream", json=payload)
    assert response.status_code == status.HTTP_200_OK
    assert response.text
    assert "bÅ‚Ä…d" not in response.text.lower()
```

### Wymagane zaleÅ¼noÅ›ci
- pytest
- pytest-asyncio
- httpx
- email-validator
- passlib
- slowapi
- aiofiles
- pyjwt

Testy uruchamiaj poleceniem:

```
docker compose exec backend poetry run pytest src/backend/tests/test_chat_endpoint.py -v
```

## âš¡ï¸ Alternatywna obsÅ‚uga vector store na GPU (PyTorch)

- DomyÅ›lnie backend uÅ¼ywa FAISS (CPU).
- Alternatywna implementacja na GPU: `src/backend/core/vector_store_gpu.py` (PyTorch, cosine similarity, wsparcie CUDA).
- Testy: `test_gpu_vector_store.py` (uruchom: `python test_gpu_vector_store.py`).
- Integracja: zamieÅ„ import na `from src.backend.core.vector_store_gpu import GPUVectorStore` i uÅ¼yj `GPUVectorStore` zamiast `VectorStore`.

--- 