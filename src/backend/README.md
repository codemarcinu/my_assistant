# FoodSave AI - Backend (FastAPI)

Nowoczesny backend dla inteligentnego asystenta spiÅ¼arni, oparty o FastAPI, Python 3.12, SQLAlchemy, Pydantic, Prometheus, OpenTelemetry, z peÅ‚nÄ… obsÅ‚ugÄ… async, testÃ³w i healthcheckÃ³w.

## ğŸš€ Szybki Start

### 1. Wymagania
- Python 3.12+
- Poetry (zalecane)
- Docker (jeÅ›li chcesz uruchomiÄ‡ caÅ‚oÅ›Ä‡ przez Compose)

### 2. Instalacja zaleÅ¼noÅ›ci
```bash
cd src/backend
poetry install
```

### 3. Uruchomienie backendu
```bash
poetry run uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 4. Testy
```bash
poetry run pytest
poetry run pytest --cov=src --cov-report=html
```

## ğŸ§ª Testowanie
- Wszystkie testy async muszÄ… mieÄ‡ dekorator `@pytest.mark.asyncio`.
- Testy jednostkowe i integracyjne w `src/backend/tests/`.
- Pokrycie kodu: `pytest --cov=src --cov-report=html`

## ğŸ—ï¸ Struktura katalogÃ³w
```
src/backend/
â”œâ”€â”€ main.py                # FastAPI app instance
â”œâ”€â”€ app_factory.py         # Tworzenie instancji FastAPI
â”œâ”€â”€ api/                   # Endpointy (routery)
â”œâ”€â”€ agents/                # Klasy agentÃ³w, orchestratory
â”œâ”€â”€ core/                  # Baza, cache, monitoring, utils
â”œâ”€â”€ models/                # SQLAlchemy + Pydantic
â”œâ”€â”€ services/              # Logika domenowa
â”œâ”€â”€ tests/                 # Testy unit/integration
â”œâ”€â”€ requirements.txt       # Dodatkowe zaleÅ¼noÅ›ci
â”œâ”€â”€ Dockerfile             # Docker backendu
â””â”€â”€ ...
```

## ğŸ³ Docker Compose
- Backend uruchamiany przez `docker-compose.yaml` (serwis: backend)
- Port domyÅ›lny: 8000
- Healthcheck: `/health`

## ğŸ”„ Monitoring i Health
- `/health` â€” ogÃ³lny health check (200 OK)
- `/ready` â€” gotowoÅ›Ä‡ do obsÅ‚ugi Å¼Ä…daÅ„
- `/metrics` â€” Prometheus metrics

## ğŸ” BezpieczeÅ„stwo
- Zmienna Å›rodowiskowa `SECRET_KEY` (w .env)
- Brak kluczy w kodzie ÅºrÃ³dÅ‚owym
- CORS: domyÅ›lnie tylko `http://localhost:3000`

## ğŸ§© NajwaÅ¼niejsze zaleÅ¼noÅ›ci
- FastAPI, Uvicorn, SQLAlchemy, Pydantic, Prometheus, OpenTelemetry, Redis, pytest, slowapi (rate limiting)

## ğŸ§° Troubleshooting
- SprawdÅº logi: `docker compose logs -f backend`
- SprawdÅº health: `curl http://localhost:8000/health`
- SprawdÅº migracje DB: `alembic upgrade head`
- SprawdÅº konfiguracjÄ™: `.env` i `config.py`

## ğŸ“„ Licencja
MIT License - zobacz [LICENSE](LICENSE) dla szczegÃ³Å‚Ã³w.

---

ZgodnoÅ›Ä‡ z `.cursorrules` i najlepszymi praktykami FastAPI.

## ğŸ”„ Streaming Responses

Backend obsÅ‚uguje streaming responses dla czatu AI z nastÄ™pujÄ…cymi endpointami:
- `/api/chat` - Podstawowy endpoint czatu ze streamingiem
- `/api/v2/chat` - Rozszerzony endpoint czatu z dodatkowymi funkcjami

### Implementacja streaming
- Asynchroniczne generatory dla kompatybilnoÅ›ci z FastAPI
- Konwersja synchronicznych generatorÃ³w LLM na asynchroniczne
- Robust pattern z threading i queue dla stabilnoÅ›ci
- ObsÅ‚uga bÅ‚Ä™dÃ³w i graceful degradation

### PrzykÅ‚ad uÅ¼ycia
```python
# Streaming response w FastAPI
async def chat_stream_generator():
    async for chunk in llm_client.stream_chat(messages):
        yield f"data: {chunk}\n\n"
``` 