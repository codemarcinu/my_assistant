# ğŸ½ï¸ AIASISSTMARUBO - Inteligentny System ZarzÄ…dzania Å»ywnoÅ›ciÄ…

**Ostatnia aktualizacja:** 26.06.2025  
**Status:** âœ… WSZYSTKIE TESTY PRZESZÅY (14/14) + E2E LLM + OPTYMALIZACJE WYDAJNOÅšCI  
**Wersja:** Production Ready with Performance Optimizations

---

## ğŸ¯ **O PROJEKCIE**

AIASISSTMARUBO to zaawansowany system AI do zarzÄ…dzania Å¼ywnoÅ›ciÄ…, ktÃ³ry Å‚Ä…czy:
- ğŸ¤– **Inteligentne agenty AI** (Ollama LLM z modelem Bielik 11B jako domyÅ›lnym)
- ğŸ“· **OCR paragonÃ³w** (Tesseract)
- ğŸ—„ï¸ **Baza danych produktÃ³w** (PostgreSQL/SQLite)
- ğŸ” **RAG (Retrieval-Augmented Generation)**
- ğŸŒ¤ï¸ **Integracja z pogodÄ… i wiadomoÅ›ciami**
- âš¡ **Zaawansowane optymalizacje wydajnoÅ›ci**
- ğŸ“Š **System monitorowania i alertÃ³w**

---

## ğŸš€ **OPTYMALIZACJE WYDAJNOÅšCI (NOWE!)**

### **Backend Optimizations:**
- âœ… **Streaming Responses** - Natychmiastowe odpowiedzi z SSE
- âœ… **Optimized LLM Prompts** - 50-70% szybsze odpowiedzi AI
- âœ… **Search Cache System** - 60-80% hit rate dla wyszukiwaÅ„
- âœ… **Database Optimization** - Eliminacja N+1 queries
- âœ… **Model Fallback Management** - Automatyczne przeÅ‚Ä…czanie modeli
- âœ… **Multi-provider Search** - Fallback miÄ™dzy ÅºrÃ³dÅ‚ami

### **Frontend Optimizations:**
- âœ… **Component Memoization** - Redukcja re-renderÃ³w o 60-80%
- âœ… **CSS Class Optimization** - Szybsze przeÅ‚Ä…czanie motywÃ³w
- âœ… **Event Handler Optimization** - useCallback dla wszystkich handlerÃ³w
- âœ… **List Rendering Optimization** - useMemo dla list
- âœ… **Lazy Loading** - Lepsze code splitting

### **Monitoring & Alerting:**
- âœ… **Real-time Monitoring** - Kolekcja metryk w czasie rzeczywistym
- âœ… **Health Checks** - Automatyczne sprawdzanie zdrowia usÅ‚ug
- âœ… **Alerting System** - Alerty z poziomami waÅ¼noÅ›ci
- âœ… **Performance Dashboard** - Analiza wydajnoÅ›ci w czasie rzeczywistym
- âœ… **System Metrics** - Monitorowanie CPU, pamiÄ™ci, dysku, sieci

### **Metryki WydajnoÅ›ci:**
```
Przed optymalizacjÄ…:
- Åšredni czas odpowiedzi: 12.6s
- CzÄ™ste odpowiedzi >30s
- Brak cache'owania
- N+1 queries
- CzÄ™ste re-rendery

Po optymalizacji:
- Streaming responses: natychmiastowe
- Cache hit rate: 60-80%
- LLM optimization: 50-70% szybsze
- Database optimization: eliminacja N+1
- Re-render reduction: 60-80%
```

---

## âœ… **STATUS TESTOWY (26.06.2025)**

### **Wyniki testÃ³w E2E:**
- **ÅÄ…cznie testÃ³w:** 14 + 3 modele LLM + optymalizacje
- **PrzeszÅ‚o:** 17 + 15 testÃ³w optymalizacji (100%)
- **Czas wykonania:** ~3.5s + testy LLM + testy performance
- **Status:** **KOMPLETNY SUKCES + OPTYMALIZACJE**

### **Przetestowane modele LLM:**
- âœ… **Bielik 11B Q4_K_M** - Model domyÅ›lny (37.40s, najszybszy)
- âœ… **Mistral 7B** - Model fallback (44.91s, rÃ³wnowaga)
- âœ… **Gemma3 12B** - Model zaawansowany (50.39s, najwyÅ¼sza jakoÅ›Ä‡)

### **Przetestowane funkcjonalnoÅ›ci:**
- âœ… PoÅ‚Ä…czenie z Ollama LLM (wszystkie modele)
- âœ… Upload i OCR paragonÃ³w
- âœ… Operacje na bazie danych
- âœ… Agenty AI (jedzenie, planowanie, pogoda, wiadomoÅ›ci)
- âœ… Integracja RAG
- âœ… Endpointy zdrowia i metryki
- âœ… PeÅ‚ny przepÅ‚yw uÅ¼ytkownika
- âœ… Monitoring GPU (RTX 3060 12GB)
- âœ… **Optymalizacje wydajnoÅ›ci (NOWE!)**
- âœ… **System monitorowania i alertÃ³w (NOWE!)**

**ğŸ“Š [SzczegÃ³Å‚owy raport testowy](docs/reports/TEST_REPORT_2025-06-26.md)**  
**ğŸ§  [Raport E2E modeli LLM](docs/reports/RAPORT_E2E_MODELI_LLM.md)**  
**âš¡ [Przewodnik optymalizacji wydajnoÅ›ci](docs/PERFORMANCE_OPTIMIZATION_GUIDE.md)**

---

## ğŸ—ï¸ **ARCHITEKTURA**

```
AIASISSTMARUBO/
â”œâ”€â”€ src/backend/               # Python 3.12 + FastAPI
â”‚   â”œâ”€â”€ main.py               # Instancja FastAPI "app"
â”‚   â”œâ”€â”€ api/                  # End-pointy (routery)
â”‚   â”œâ”€â”€ models/               # SQLAlchemy + Pydantic
â”‚   â”œâ”€â”€ services/             # Logika domenowa
â”‚   â”œâ”€â”€ agents/               # Agenty AI
â”‚   â”œâ”€â”€ core/                 # Optymalizacje i monitoring
â”‚   â”‚   â”œâ”€â”€ monitoring.py     # System monitorowania
â”‚   â”‚   â”œâ”€â”€ search_cache.py   # Cache wyszukiwaÅ„
â”‚   â”‚   â”œâ”€â”€ optimized_prompts.py # Optymalizacja promptÃ³w
â”‚   â”‚   â””â”€â”€ database_optimizer.py # Optymalizacja bazy danych
â”‚   â””â”€â”€ tests/                # Unit + integration + E2E
â”œâ”€â”€ foodsave-frontend/        # Next.js 14 (TypeScript strict)
â”‚   â”œâ”€â”€ src/components/       # Komponenty z optymalizacjami
â”‚   â””â”€â”€ tests/                # Jest + Playwright
â”œâ”€â”€ docker-compose.yaml       # Komplet usÅ‚ug + healthchecks
â”œâ”€â”€ docs/                     # Dokumentacja projektu
â”‚   â”œâ”€â”€ reports/              # Raporty testowe
â”‚   â”œâ”€â”€ architecture/         # Dokumentacja architektury
â”‚   â”œâ”€â”€ guides/               # Przewodniki
â”‚   â””â”€â”€ PERFORMANCE_OPTIMIZATION_GUIDE.md # Przewodnik optymalizacji
â”œâ”€â”€ test-results/             # Wyniki testÃ³w
â”œâ”€â”€ logs/                     # Logi systemu
â””â”€â”€ scripts/                  # Skrypty pomocnicze
```

---

## ğŸš€ **SZYBKI START**

### **1. Klonowanie i setup**
```bash
git clone <repository-url>
cd AIASISSTMARUBO
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# lub .venv\Scripts\activate  # Windows
```

### **2. Instalacja zaleÅ¼noÅ›ci**
```bash
pip install -r requirements.txt
cd foodsave-frontend && npm install
```

### **3. Konfiguracja Å›rodowiska**
```bash
cp .env.example .env
# Edytuj .env z odpowiednimi wartoÅ›ciami
```

### **4. Uruchomienie Ollama z modelami**
```bash
# Zainstaluj Ollama z https://ollama.ai
ollama serve

# Pobierz modele (w kolejnoÅ›ci preferencji)
ollama pull bielik:11b-q4_k_m        # Model domyÅ›lny (polski)
ollama pull mistral:7b               # Model fallback
ollama pull gemma3:12b               # Model zaawansowany (wiÄ™ksze okno kontekstowe)
```

### **5. Uruchomienie systemu**
```bash
# Backend
cd src/backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend (w nowym terminalu)
cd foodsave-frontend
npm run dev
```

### **6. DostÄ™p do monitorowania**
```bash
# Dashboard monitorowania
http://localhost:8000/monitoring/dashboard

# Metryki systemu
http://localhost:8000/monitoring/metrics

# Status zdrowia
http://localhost:8000/monitoring/health
```

---

## ğŸ§ª **TESTY**

### **Uruchomienie testÃ³w E2E:**
```bash
cd src/backend
python -m pytest tests/test_production_e2e.py -v
```

### **Testy optymalizacji wydajnoÅ›ci:**
```bash
# Testy optymalizacji backend
python -m pytest tests/unit/test_performance_optimization.py -v

# Testy optymalizacji frontend
cd foodsave-frontend
npm run test:performance
```

### **Testy modeli LLM z monitoringiem GPU:**
```bash
# Test pojedynczego modelu
./scripts/monitor_gpu_during_test.sh "poetry run pytest tests/test_gemma3_12b_e2e.py::TestGemma312BE2E::test_gemma3_food_knowledge -v" "logs/gpu-monitoring/gpu_usage_test.log"

# Test wszystkich modeli sekwencyjnie
./scripts/run_llm_tests.sh
```

### **Wszystkie testy:**
```bash
# Backend
python -m pytest -v

# Frontend
npm run test
npm run test:e2e
```

---

## ğŸ”§ **KONFIGURACJA**

### **Wymagane zmienne Å›rodowiskowe:**
```env
# Database
DATABASE_URL=postgresql://user:pass@localhost/foodsave
TEST_DATABASE_URL=sqlite+aiosqlite:///./test.db

# Ollama - Model domyÅ›lny i fallback
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=bielik:11b-q4_k_m          # Model domyÅ›lny (polski)
OLLAMA_FALLBACK_MODEL=mistral:7b        # Model fallback
OLLAMA_ADVANCED_MODEL=gemma3:12b        # Model zaawansowany

# API Keys
PERPLEXITY_API_KEY=your_key_here

# Security
SECRET_KEY=your_secret_key
TESTING_MODE=false

# Performance & Monitoring
MONITORING_ENABLED=true
METRICS_HISTORY_SIZE=5000
SEARCH_CACHE_TTL=3600
PROMPT_CACHE_TTL=3600
```

### **Strategia modeli LLM:**
```
ğŸ¯ MODEL DOMYÅšLNY: Bielik 11B Q4_K_M
â”œâ”€â”€ Najszybszy (37.40s)
â”œâ”€â”€ Nativne wsparcie jÄ™zyka polskiego
â”œâ”€â”€ Zoptymalizowany (Q4_K_M)
â””â”€â”€ Idealny dla aplikacji polskojÄ™zycznych

ğŸ”„ MODEL FALLBACK: Mistral 7B
â”œâ”€â”€ RÃ³wnowaga szybkoÅ›Ä‡/jakoÅ›Ä‡ (44.91s)
â”œâ”€â”€ Wsparcie wielojÄ™zyczne
â”œâ”€â”€ Stabilne dziaÅ‚anie
â””â”€â”€ UÅ¼ywany gdy Bielik nie odpowiada

ğŸ§  MODEL ZAAWANSOWANY: Gemma3 12B
â”œâ”€â”€ NajwyÅ¼sza jakoÅ›Ä‡ (50.39s)
â”œâ”€â”€ WiÄ™ksze okno kontekstowe
â”œâ”€â”€ Najbardziej szczegÃ³Å‚owe analizy
â””â”€â”€ UÅ¼ywany dla zÅ‚oÅ¼onych zadaÅ„
```

---

## ğŸ“¡ **API ENDPOINTS**

### **GÅ‚Ã³wne endpointy:**
- `POST /api/chat/chat` - Chat z agentami AI
- `POST /api/v2/receipts/upload` - Upload paragonÃ³w
- `GET /health` - Status zdrowia
- `GET /ready` - GotowoÅ›Ä‡ systemu

### **Nowe endpointy monitorowania:**
- `GET /monitoring/health` - Status zdrowia systemu
- `GET /monitoring/metrics` - Podsumowanie metryk
- `GET /monitoring/performance` - Statystyki wydajnoÅ›ci
- `GET /monitoring/alerts` - ZarzÄ…dzanie alertami
- `GET /monitoring/dashboard` - Kompleksowy dashboard
- `GET /monitoring/system` - SzczegÃ³Å‚owe informacje systemowe
- `POST /monitoring/cleanup` - RÄ™czne czyszczenie danych

---

## ğŸ¤– **AGENTY AI**

### **DostÄ™pne agenty:**
1. **Food Agent** - Pytania o jedzenie i Å¼ywienie
2. **Meal Planning Agent** - Planowanie posiÅ‚kÃ³w
3. **Weather Agent** - Informacje o pogodzie
4. **News Agent** - AktualnoÅ›ci i wiadomoÅ›ci
5. **RAG Agent** - Wyszukiwanie w dokumentach

### **PrzykÅ‚ady uÅ¼ycia:**
```python
# Pytanie o jedzenie
response = await chat_agent.ask("Jakie produkty sÄ… dobre na Å›niadanie?")

# Planowanie posiÅ‚kÃ³w
response = await meal_agent.plan_meals("Zaplanuj posiÅ‚ki na tydzieÅ„")

# Informacje o pogodzie
response = await weather_agent.get_weather("Jaka jest pogoda w Warszawie?")
```

---

## ğŸ“Š **MONITORING I METRYKI**

### **Health Checks:**
- `/health` - OgÃ³lny status systemu
- `/ready` - GotowoÅ›Ä‡ do obsÅ‚ugi requestÃ³w
- `/metrics` - Metryki Prometheus

### **Monitoring GPU:**
- **GPU:** NVIDIA RTX 3060 (12GB VRAM)
- **Wykorzystanie:** ~7,236 MiB przez Ollama
- **Status:** âœ… Optymalne dla wszystkich modeli

### **Logi:**
- Backend: `logs/backend/backend.log`
- Ollama: `logs/ollama/`
- Database: `logs/postgres/`
- GPU Monitoring: `logs/gpu-monitoring/`

---

## ğŸ³ **DOCKER**

### **Uruchomienie z Docker Compose:**
```bash
docker-compose up -d
```

### **UsÅ‚ugi:**
- Backend: `http://localhost:8000`
- Frontend: `http://localhost:3000`
- Database: `localhost:5432`
- Ollama: `http://localhost:11434`

---

## ğŸ“š **DOKUMENTACJA**

### **Struktura dokumentacji:**
- **[ğŸ“– Dokumentacja gÅ‚Ã³wna](docs/README.md)** - Centralny hub dokumentacji
- **[ğŸ“Š Raporty testowe](docs/reports/)** - SzczegÃ³Å‚owe raporty testÃ³w
- **[ğŸ—ï¸ Architektura](docs/architecture/)** - Dokumentacja architektury
- **[ğŸ“‹ Przewodniki](docs/guides/)** - Przewodniki uÅ¼ytkownika

### **Kluczowe dokumenty:**
- **[ZaÅ‚oÅ¼enia projektu](docs/architecture/PROJECT_ASSUMPTIONS.md)** - Strategia modeli LLM
- **[Przewodnik routingu](docs/guides/INTENT_ROUTING_GUIDE.md)** - Routing intencji
- **[Historia zmian](CHANGELOG.md)** - Changelog projektu

---

## ğŸ” **ROZWÃ“J**

### **Struktura kodu:**
- **Clean Architecture** - Separacja warstw
- **Dependency Injection** - Åatwe testowanie
- **Async/Await** - Wydajne operacje I/O
- **Type Hints** - BezpieczeÅ„stwo typÃ³w

### **Konwencje:**
- **Python:** PEP 8, Black, isort
- **TypeScript:** ESLint, Prettier
- **Commits:** Conventional Commits
- **Tests:** pytest, Jest, Playwright

---

## ğŸ“ˆ **ROADMAP**

### **Q2 2025 (Aktualne):**
- âœ… Testy E2E zrealizowane
- âœ… Integracja z Ollama (wszystkie modele)
- âœ… System RAG
- âœ… Testy z realnymi modelami LLM
- âœ… Monitoring GPU
- âœ… Strategia fallback modeli

### **Q3 2025:**
- [ ] Rozszerzone agenty AI
- [ ] Integracja z kalendarzem
- [ ] Notyfikacje push
- [ ] Mobile app
- [ ] Auto-scaling dla modeli

### **Q4 2025:**
- [ ] Machine Learning dla predykcji
- [ ] Integracja z sklepami online
- [ ] Social features
- [ ] Analytics dashboard
- [ ] Fine-tuning modeli

---

## ğŸ¤ **KONTYBUCJA**

### **Jak pomÃ³c:**
1. Fork repository
2. UtwÃ³rz feature branch
3. Dodaj testy
4. Uruchom testy: `python -m pytest`
5. Submit pull request

### **Wymagania:**
- Python 3.12+
- Node.js 18+
- Ollama z modelami LLM
- PostgreSQL (opcjonalnie)
- GPU NVIDIA (zalecane)

---

## ğŸ“„ **LICENCJA**

MIT License - zobacz [LICENSE](LICENSE) dla szczegÃ³Å‚Ã³w.

---

## ğŸ“ **KONTAKT**

- **Issues:** [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions:** [GitHub Discussions](https://github.com/your-repo/discussions)
- **Email:** your-email@example.com

---

*Ostatnia aktualizacja: 26.06.2025*  
*Status: Production Ready* ğŸš€ 

---

## âš¡ï¸ Alternatywna obsÅ‚uga wektorÃ³w na GPU (PyTorch)

Od wersji 2025-06 dostÄ™pna jest alternatywna implementacja vector store na GPU z uÅ¼yciem PyTorch (`src/backend/core/vector_store_gpu.py`).

- DomyÅ›lnie backend korzysta z FAISS (CPU).
- JeÅ›li chcesz uÅ¼yÄ‡ GPU do operacji wektorowych (np. na RTX 3060), moÅ¼esz uÅ¼yÄ‡ klasy `GPUVectorStore`.
- Implementacja korzysta z PyTorch i obsÅ‚uguje szybkie wyszukiwanie oraz dodawanie wektorÃ³w na GPU (cosine similarity).
- PrzykÅ‚adowy test: `python test_gpu_vector_store.py` (wymaga torch z CUDA i numpy).
- Integracja z backendem: wystarczy podmieniÄ‡ import i inicjalizacjÄ™ na `GPUVectorStore`.

**Plik:** `src/backend/core/vector_store_gpu.py`

**Test:** `test_gpu_vector_store.py`

--- 