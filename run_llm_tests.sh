#!/bin/bash

# ğŸ§  Skrypt do uruchamiania testÃ³w E2E wszystkich modeli LLM
# Autor: AI Assistant
# Data: 26.06.2025

set -e  # Zatrzymaj na bÅ‚Ä™dzie

echo "ğŸš€ URUCHAMIANIE TESTÃ“W E2E MODELI LLM"
echo "====================================="
echo ""

# SprawdÅº czy backend dziaÅ‚a
echo "ğŸ” Sprawdzanie statusu backendu..."
if ! curl -s http://localhost:8000/health > /dev/null; then
    echo "âŒ Backend nie odpowiada na http://localhost:8000/health"
    echo "   Uruchom backend: uvicorn src.backend.main:app --host 0.0.0.0 --port 8000"
    exit 1
fi
echo "âœ… Backend dziaÅ‚a poprawnie"

# SprawdÅº czy Ollama dziaÅ‚a
echo "ğŸ” Sprawdzanie statusu Ollama..."
if ! curl -s http://localhost:11434/api/version > /dev/null; then
    echo "âŒ Ollama nie odpowiada na http://localhost:11434"
    echo "   Uruchom Ollama: ollama serve"
    exit 1
fi
echo "âœ… Ollama dziaÅ‚a poprawnie"

# UtwÃ³rz katalog na logi jeÅ›li nie istnieje
mkdir -p logs/llm_tests

# Funkcja do uruchamiania testu z monitoringiem GPU
run_test() {
    local model_name=$1
    local test_name=$2
    local log_file="logs/llm_tests/gpu_usage_${model_name}_$(date +%Y%m%d_%H%M%S).log"
    
    echo ""
    echo "ğŸ§ª TEST: $model_name"
    echo "ğŸ“ Nazwa testu: $test_name"
    echo "ğŸ“Š Log GPU: $log_file"
    echo "â±ï¸  Rozpoczynam test..."
    
    # Uruchom test z monitoringiem GPU
    ./monitor_gpu_during_test.sh \
        "poetry run pytest src/backend/tests/test_gemma3_12b_e2e.py::TestGemma312BE2E::$test_name -v --tb=short" \
        "$log_file"
    
    # SprawdÅº wynik testu
    if [ $? -eq 0 ]; then
        echo "âœ… Test $model_name zakoÅ„czony sukcesem"
    else
        echo "âŒ Test $model_name zakoÅ„czony bÅ‚Ä™dem"
    fi
    
    echo "ğŸ“Š Analiza logÃ³w GPU..."
    if [ -f "$log_file" ]; then
        echo "   Rozmiar logu: $(du -h "$log_file" | cut -f1)"
        echo "   Ostatnie linie:"
        tail -3 "$log_file" | sed 's/^/   /'
    fi
    
    echo "â³ Czekam 30 sekund przed nastÄ™pnym testem..."
    sleep 30
}

# Uruchom testy sekwencyjnie
echo ""
echo "ğŸ¯ ROZPOCZYNAM TESTY SEKWENCYJNE"
echo "================================"

# Test 1: Bielik 11B Q4_K_M (model domyÅ›lny)
run_test "bielik_11b" "test_gemma3_food_knowledge"

# Test 2: Mistral 7B (model fallback)
run_test "mistral_7b" "test_gemma3_food_knowledge"

# Test 3: Gemma3 12B (model zaawansowany)
run_test "gemma3_12b" "test_gemma3_food_knowledge"

echo ""
echo "ğŸ‰ WSZYSTKIE TESTY ZAKOÅƒCZONE"
echo "============================="
echo ""

# Podsumowanie wynikÃ³w
echo "ğŸ“Š PODSUMOWANIE WYNIKÃ“W:"
echo "========================"

# SprawdÅº pliki wynikÃ³w
echo "ğŸ“‹ Pliki wynikÃ³w JSON:"
ls -la test_results_*_$(date +%Y%m%d)*.json 2>/dev/null || echo "   Brak plikÃ³w wynikÃ³w"

echo ""
echo "ğŸ“ˆ Pliki logÃ³w GPU:"
ls -la logs/llm_tests/gpu_usage_*_$(date +%Y%m%d)*.log 2>/dev/null || echo "   Brak plikÃ³w logÃ³w"

echo ""
echo "ğŸ” Status systemu:"
echo "   Backend: $(curl -s http://localhost:8000/health | jq -r '.status' 2>/dev/null || echo 'N/A')"
echo "   Ollama: $(curl -s http://localhost:11434/api/version | jq -r '.version' 2>/dev/null || echo 'N/A')"

echo ""
echo "ğŸ“ NastÄ™pne kroki:"
echo "   1. SprawdÅº szczegÃ³Å‚owy raport: RAPORT_E2E_MODELI_LLM.md"
echo "   2. Przeanalizuj logi GPU w katalogu logs/llm_tests/"
echo "   3. SprawdÅº pliki wynikÃ³w JSON"
echo "   4. Zaktualizuj dokumentacjÄ™ jeÅ›li potrzeba"

echo ""
echo "âœ… Testy E2E modeli LLM zakoÅ„czone!" 