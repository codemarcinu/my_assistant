# üß™ Przewodnik po Testowaniu - FoodSave AI

## üìã Strategia Testowania

FoodSave AI wykorzystuje kompleksowƒÖ strategiƒô testowania obejmujƒÖcƒÖ testy jednostkowe, integracyjne, wydajno≈õciowe i end-to-end. System testowania jest zoptymalizowany pod kƒÖtem szybko≈õci, niezawodno≈õci i pokrycia kodu.

## üéØ Typy Test√≥w

### 1. Unit Tests (Testy Jednostkowe)

Testy pojedynczych komponent√≥w w izolacji.

**Lokalizacja:** `tests/unit/`

**Przyk≈Çady:**
- Testy agent√≥w AI
- Testy serwis√≥w
- Testy modeli danych
- Testy utility functions

### 2. Integration Tests (Testy Integracyjne)

Testy interakcji miƒôdzy komponentami.

**Lokalizacja:** `tests/integration/`

**Przyk≈Çady:**
- Testy API endpoints
- Testy komunikacji z bazƒÖ danych
- Testy integracji z zewnƒôtrznymi API
- Testy orkiestracji agent√≥w

### 3. Performance Tests (Testy Wydajno≈õciowe)

Testy wydajno≈õci i obciƒÖ≈ºenia.

**Lokalizacja:** `tests/performance/`

**Przyk≈Çady:**
- Testy response time
- Testy memory usage
- Load testing
- Stress testing

### 4. E2E Tests (Testy End-to-End)

Testy kompletnych scenariuszy u≈ºytkownika.

**Lokalizacja:** `tests/e2e/`

**Przyk≈Çady:**
- Testy przep≈Çyw√≥w u≈ºytkownika
- Testy interfejsu u≈ºytkownika
- Testy kompletnych funkcjonalno≈õci

## üé® Frontend Testing (React/TypeScript)

### Status: ‚úÖ **KOMPLETNE POKRYCIE TESTAMI**

Frontend aplikacji FoodSave AI ma kompletny zestaw test√≥w z 100% pokryciem g≈Ç√≥wnych komponent√≥w i hook√≥w.

#### üìä Wyniki Test√≥w Frontend

| Komponent/Hook | Testy | Status | Pokrycie |
|----------------|-------|--------|----------|
| **ErrorBanner** | 18/18 | ‚úÖ PASS | 100% |
| **useWebSocket** | 26/26 | ‚úÖ PASS | 100% |
| **useRAG** | 20/20 | ‚úÖ PASS | 100% |
| **useTauriAPI** | 9/9 | ‚úÖ PASS | 100% |
| **TauriTestComponent** | 8/8 | ‚úÖ PASS | 100% |
| **≈ÅƒÑCZNIE** | **81/81** | **‚úÖ PASS** | **100%** |

#### üõ†Ô∏è Stack Technologiczny Frontend

- **Jest:** Framework testowy
- **React Testing Library:** Testowanie komponent√≥w React
- **@testing-library/user-event:** Symulacja interakcji u≈ºytkownika
- **@testing-library/jest-dom:** Dodatkowe matchery

#### üìÅ Struktura Test√≥w Frontend

```
myappassistant-chat-frontend/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ unit/
‚îÇ       ‚îú‚îÄ‚îÄ components/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ErrorBanner.test.tsx
‚îÇ       ‚îî‚îÄ‚îÄ hooks/
‚îÇ           ‚îú‚îÄ‚îÄ useWebSocket.test.ts
‚îÇ           ‚îî‚îÄ‚îÄ useRAG.test.ts
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/__tests__/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TauriTestComponent.test.tsx
‚îÇ   ‚îî‚îÄ‚îÄ hooks/__tests__/
‚îÇ       ‚îî‚îÄ‚îÄ useTauriAPI.test.ts
‚îî‚îÄ‚îÄ jest.config.js
```

#### üöÄ Uruchamianie Test√≥w Frontend

```bash
# Przejd≈∫ do katalogu frontend
cd myappassistant-chat-frontend

# Wszystkie testy
npm test

# Konkretny plik testowy
npm test -- ErrorBanner.test.tsx

# Testy z coverage
npm test -- --coverage

# Testy w trybie watch
npm test -- --watch
```

#### üìù Przyk≈Çady Test√≥w Frontend

**1. Test Komponentu React**

```typescript
// tests/unit/components/ErrorBanner.test.tsx
import { render, screen, fireEvent } from '@testing-library/react';
import { ErrorBanner } from '@/components/ErrorBanner';

describe('ErrorBanner', () => {
  const defaultProps = {
    error: 'Test error message',
    onRetry: jest.fn(),
    onDismiss: jest.fn(),
  };

  it('should render error message', () => {
    render(<ErrorBanner {...defaultProps} />);
    expect(screen.getByText('Test error message')).toBeInTheDocument();
  });

  it('should call onRetry when retry button is clicked', () => {
    const onRetry = jest.fn();
    render(<ErrorBanner {...defaultProps} onRetry={onRetry} />);
    
    fireEvent.click(screen.getByRole('button', { name: /retry/i }));
    expect(onRetry).toHaveBeenCalledTimes(1);
  });
});
```

**2. Test Hook React**

```typescript
// tests/unit/hooks/useWebSocket.test.ts
import { renderHook, act } from '@testing-library/react';
import { useWebSocket } from '@/hooks/useWebSocket';

describe('useWebSocket', () => {
  beforeEach(() => {
    // Mock WebSocket
    global.WebSocket = jest.fn(() => ({
      addEventListener: jest.fn(),
      removeEventListener: jest.fn(),
      send: jest.fn(),
      close: jest.fn(),
      readyState: WebSocket.OPEN,
    })) as any;
  });

  it('should connect to WebSocket on mount', () => {
    renderHook(() => useWebSocket());
    expect(global.WebSocket).toHaveBeenCalledWith('ws://localhost:8000/ws/dashboard');
  });

  it('should handle connection errors', () => {
    const { result } = renderHook(() => useWebSocket());
    
    act(() => {
      // Simulate connection error
      const ws = global.WebSocket.mock.results[0].value;
      ws.onerror(new Event('error'));
    });

    expect(result.current.error).toBe('WebSocket connection error');
  });
});
```

#### üéØ Najlepsze Praktyki Frontend

1. **Arrange-Act-Assert:** Czytelna struktura test√≥w
2. **Mocking:** Izolacja testowanych jednostek
3. **Accessibility:** Testy dostƒôpno≈õci dla wszystkich komponent√≥w
4. **Error Handling:** Testy obs≈Çugi b≈Çƒôd√≥w i edge cases
5. **Async Testing:** Prawid≈Çowe testowanie operacji asynchronicznych

#### üîß Konfiguracja Jest

```javascript
// jest.config.js
module.exports = {
  testEnvironment: 'jsdom',
  setupFilesAfterEnv: ['<rootDir>/src/setupTests.ts'],
  moduleNameMapping: {
    '^@/(.*)$': '<rootDir>/src/$1',
  },
  collectCoverageFrom: [
    'src/**/*.{ts,tsx}',
    '!src/**/*.d.ts',
    '!src/index.tsx',
  ],
};
```

## üõ†Ô∏è Konfiguracja Test√≥w Backend

### Pytest Configuration

```python
# pyproject.toml
[tool.pytest.ini_options]
pythonpath = ["src"]
markers = [
    "e2e: marks tests as end-to-end",
    "performance: marks tests as performance tests",
    "memory: marks tests as memory profiling tests",
    "slow: marks tests as slow running",
    "integration: marks tests as integration tests",
    "concise: marks tests as concise response tests",
]

[tool.pytest-benchmark]
min_rounds = 5
max_time = 10.0
warmup = true
```

### Test Dependencies

```toml
[tool.poetry.group.dev.dependencies]
pytest = "^8.0"
pytest-cov = "^4.0"
pytest-asyncio = "^0.20.0"
pytest-benchmark = "^4.0.0"
pytest-mock = "^3.12.0"
pytest-xdist = "^3.5.0"
memory-profiler = "^0.61.0"
memray = "^1.12.0"
locust = "^2.20.0"
```

## üß™ Przyk≈Çady Test√≥w Backend

### 1. Unit Tests - Agent Tests

```python
# tests/unit/test_chef_agent.py
import pytest
from unittest.mock import AsyncMock, patch
from src.backend.agents.chef_agent import ChefAgent

class TestChefAgent:
    """Testy dla Chef Agent"""

    @pytest.fixture
    def mock_llm_client(self):
        return AsyncMock()

    @pytest.fixture
    def mock_database(self):
        return AsyncMock()

    @pytest.fixture
    def chef_agent(self, mock_llm_client, mock_database):
        return ChefAgent(mock_llm_client, mock_database)

    @pytest.mark.asyncio
    async def test_recipe_generation(self, chef_agent, mock_llm_client):
        """Test generowania przepisu"""
        # Arrange
        mock_llm_client.chat.return_value = {
            "message": {"content": "Przepis na pizzƒô: sk≈Çadniki..."}
        }

        # Act
        response = await chef_agent.process("Poka≈º mi przepis na pizzƒô", {})

        # Assert
        assert "pizza" in response.lower()
        assert "sk≈Çadniki" in response.lower()
        mock_llm_client.chat.assert_called_once()

    @pytest.mark.asyncio
    async def test_ingredient_substitution(self, chef_agent, mock_llm_client):
        """Test sugestii substytut√≥w sk≈Çadnik√≥w"""
        # Arrange
        mock_llm_client.chat.return_value = {
            "message": {"content": "Mo≈ºesz zastƒÖpiƒá mleko mlekiem migda≈Çowym..."}
        }

        # Act
        response = await chef_agent.process("Czy mogƒô zastƒÖpiƒá mleko?", {})

        # Assert
        assert "substytut" in response.lower() or "zastƒÖpiƒá" in response.lower()
        assert "migda≈Çowe" in response.lower()

    @pytest.mark.asyncio
    async def test_error_handling(self, chef_agent, mock_llm_client):
        """Test obs≈Çugi b≈Çƒôd√≥w"""
        # Arrange
        mock_llm_client.chat.side_effect = Exception("LLM error")

        # Act & Assert
        with pytest.raises(Exception):
            await chef_agent.process("Poka≈º mi przepis", {})
```

### 2. Unit Tests - Concise Response Tests

```python
# tests/unit/test_concise_responses.py
import pytest
from unittest.mock import AsyncMock, patch
from src.backend.agents.concise_response_agent import ConciseResponseAgent
from src.backend.core.response_length_config import ResponseLengthConfig, ResponseStyle
from src.backend.core.concise_rag_processor import ConciseRAGProcessor

class TestConciseResponseAgent:
    """Testy dla Concise Response Agent"""

    @pytest.fixture
    def mock_llm_client(self):
        return AsyncMock()

    @pytest.fixture
    def mock_vector_store(self):
        return AsyncMock()

    @pytest.fixture
    def concise_agent(self, mock_llm_client, mock_vector_store):
        return ConciseResponseAgent(mock_llm_client, mock_vector_store)

    @pytest.mark.asyncio
    async def test_concise_generation(self, concise_agent, mock_llm_client):
        """Test generowania zwiƒôz≈Çej odpowiedzi"""
        # Arrange
        mock_llm_client.chat.return_value = {
            "message": {"content": "Sunny, 25¬∞C with light breeze."}
        }

        # Act
        response = await concise_agent.process({
            "query": "What is the weather today?",
            "style": "concise"
        })

        # Assert
        assert response.response is not None
        assert response.style == "concise"
        assert response.conciseness_score > 0.8
        assert len(response.response.split()) <= 10

    @pytest.mark.asyncio
    async def test_response_expansion(self, concise_agent, mock_llm_client):
        """Test rozszerzania zwiƒôz≈Çej odpowiedzi"""
        # Arrange
        concise_text = "Sunny, 25¬∞C"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Today's weather is sunny with a temperature of 25¬∞C..."}
        }

        # Act
        expanded = await concise_agent.expand_response(concise_text, "What is the weather?")

        # Assert
        assert len(expanded) > len(concise_text)
        assert "25¬∞C" in expanded
        assert expanded.count(".") > 1

    @pytest.mark.asyncio
    async def test_rag_integration(self, concise_agent, mock_vector_store, mock_llm_client):
        """Test integracji z RAG"""
        # Arrange
        mock_vector_store.search.return_value = [
            {"content": "Weather data shows sunny conditions..."}
        ]
        mock_llm_client.chat.return_value = {
            "message": {"content": "Based on weather data: Sunny, 25¬∞C"}
        }

        # Act
        response = await concise_agent.process({
            "query": "What is the weather?",
            "style": "concise",
            "use_rag": True
        })

        # Assert
        assert response.rag_used is True
        mock_vector_store.search.assert_called_once()

class TestResponseLengthConfig:
    """Testy dla konfiguracji d≈Çugo≈õci odpowiedzi"""

    def test_concise_config(self):
        """Test konfiguracji zwiƒôz≈Çego stylu"""
        config = ResponseLengthConfig(ResponseStyle.CONCISE)
        
        assert config.max_tokens <= 100
        assert config.temperature <= 0.3
        assert config.num_predict <= 60

    def test_standard_config(self):
        """Test konfiguracji standardowego stylu"""
        config = ResponseLengthConfig(ResponseStyle.STANDARD)
        
        assert 100 < config.max_tokens <= 500
        assert 0.3 < config.temperature <= 0.6

    def test_detailed_config(self):
        """Test konfiguracji szczeg√≥≈Çowego stylu"""
        config = ResponseLengthConfig(ResponseStyle.DETAILED)
        
        assert config.max_tokens > 500
        assert config.temperature > 0.6

    def test_ollama_options(self):
        """Test generowania opcji Ollama"""
        config = ResponseLengthConfig(ResponseStyle.CONCISE)
        options = config.get_ollama_options()
        
        assert "num_predict" in options
        assert "temperature" in options
        assert "top_k" in options
        assert "top_p" in options

class TestConciseRAGProcessor:
    """Testy dla procesora RAG zwiƒôz≈Çych odpowiedzi"""

    @pytest.fixture
    def mock_llm_client(self):
        return AsyncMock()

    @pytest.fixture
    def processor(self, mock_llm_client):
        return ConciseRAGProcessor(mock_llm_client)

    @pytest.mark.asyncio
    async def test_map_reduce_processing(self, processor, mock_llm_client):
        """Test dwustopniowego przetwarzania map-reduce"""
        # Arrange
        chunks = [
            {"content": "Weather data shows sunny conditions"},
            {"content": "Temperature is 25¬∞C"},
            {"content": "Light breeze from northwest"}
        ]
        mock_llm_client.chat.side_effect = [
            {"message": {"content": "Sunny conditions"}},
            {"message": {"content": "25¬∞C temperature"}},
            {"message": {"content": "Light breeze"}},
            {"message": {"content": "Sunny, 25¬∞C with light breeze"}}
        ]

        # Act
        result = await processor.process_with_map_reduce("What is the weather?", chunks)

        # Assert
        assert "Sunny" in result
        assert "25¬∞C" in result
        assert mock_llm_client.chat.call_count == 4  # 3 summaries + 1 final

    @pytest.mark.asyncio
    async def test_summary_length_control(self, processor, mock_llm_client):
        """Test kontroli d≈Çugo≈õci podsumowa≈Ñ"""
        # Arrange
        chunks = [{"content": "Very long weather description..." * 10}]
        mock_llm_client.chat.return_value = {
            "message": {"content": "Short summary"}
        }

        # Act
        result = await processor.process_with_map_reduce("Weather?", chunks, max_summary_length=50)

        # Assert
        assert len(result) <= 200  # Final response should be concise
```

### 3. Unit Tests - Anti-Hallucination Tests

```python
# tests/unit/test_anti_hallucination.py
import pytest
from unittest.mock import AsyncMock, patch
from src.backend.agents.general_conversation_agent import GeneralConversationAgent

class TestAntiHallucinationSystem:
    """Testy dla systemu anty-halucynacyjnego"""

    @pytest.fixture
    def mock_llm_client(self):
        return AsyncMock()

    @pytest.fixture
    def general_agent(self, mock_llm_client):
        return GeneralConversationAgent(mock_llm_client)

    @pytest.mark.asyncio
    async def test_fictional_character_blocking(self, general_agent, mock_llm_client):
        """Test blokowania fikcyjnych postaci"""
        # Arrange
        query = "Tell me about Jan Kowalski, a Polish scientist from the 19th century"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Jan Kowalski was a Polish chemist who was born in..."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Nie mam informacji o tej osobie" in response
        assert "Jan Kowalski" not in response

    @pytest.mark.asyncio
    async def test_fictional_product_blocking(self, general_agent, mock_llm_client):
        """Test blokowania fikcyjnych produkt√≥w"""
        # Arrange
        query = "What are the specifications of Samsung Galaxy XYZ 2025?"
        mock_llm_client.chat.return_value = {
            "message": {"content": "The Samsung Galaxy XYZ 2025 features a 6.7-inch display..."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Nie mam informacji o tym produkcie" in response
        assert "Samsung Galaxy XYZ 2025" not in response

    @pytest.mark.asyncio
    async def test_known_person_allowed(self, general_agent, mock_llm_client):
        """Test pozwalania na znane osoby"""
        # Arrange
        query = "Who is the current president of Poland?"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Andrzej Duda is the current president of Poland."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Andrzej Duda" in response
        assert "Nie mam informacji" not in response

    @pytest.mark.asyncio
    async def test_biographical_pattern_detection(self, general_agent, mock_llm_client):
        """Test wykrywania wzorc√≥w biograficznych"""
        # Arrange
        query = "Tell me about Anna Nowak, a famous Polish chemist"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Anna Nowak was a Polish chemist who was born in 1850..."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Nie mam informacji o tej osobie" in response
        assert "urodzi≈Ç siƒô" not in response

    @pytest.mark.asyncio
    async def test_product_specification_detection(self, general_agent, mock_llm_client):
        """Test wykrywania specyfikacji produkt√≥w"""
        # Arrange
        query = "What are the specs of iPhone Future 2026?"
        mock_llm_client.chat.return_value = {
            "message": {"content": "The iPhone Future 2026 has a 7.2-inch display with 8GB RAM..."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Nie mam informacji o tym produkcie" in response
        assert "RAM" not in response

    @pytest.mark.asyncio
    async def test_fuzzy_name_matching(self, general_agent, mock_llm_client):
        """Test fuzzy matching nazw"""
        # Arrange
        query = "Who is Piotr Wi≈õniewski?"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Piotr Wi≈õniewski was a Polish mathematician..."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Nie mam informacji o tej osobie" in response

    @pytest.mark.asyncio
    async def test_whitelist_functionality(self, general_agent, mock_llm_client):
        """Test funkcjonalno≈õci whitelist"""
        # Arrange
        query = "Tell me about Andrzej Duda"
        mock_llm_client.chat.return_value = {
            "message": {"content": "Andrzej Duda is the current president of Poland."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "Andrzej Duda" in response
        assert "Nie mam informacji" not in response

    @pytest.mark.asyncio
    async def test_general_query_allowed(self, general_agent, mock_llm_client):
        """Test pozwalania na og√≥lne zapytania"""
        # Arrange
        query = "What's the weather like today?"
        mock_llm_client.chat.return_value = {
            "message": {"content": "I don't have access to current weather information."}
        }

        # Act
        response = await general_agent.process_query(query, {})

        # Assert
        assert "weather" in response.lower()
        assert "Nie mam informacji" not in response

    @pytest.mark.asyncio
    async def test_temperature_optimization(self, general_agent, mock_llm_client):
        """Test optymalizacji temperatury"""
        # Act
        await general_agent.process_query("Test query", {})

        # Assert
        call_args = mock_llm_client.chat.call_args
        options = call_args[1].get('options', {})
        assert options.get('temperature') == 0.1

    @pytest.mark.asyncio
    async def test_system_prompt_enhancement(self, general_agent, mock_llm_client):
        """Test ulepszonego system prompt"""
        # Act
        await general_agent.process_query("Test query", {})

        # Assert
        call_args = mock_llm_client.chat.call_args
        messages = call_args[0][0]
        system_message = messages[0]['content']
        assert "KRYTYCZNE ZASADY PRZECIWKO HALLUCINACJOM" in system_message
        assert "NIGDY nie wymy≈õlaj fakt√≥w" in system_message
```

### 4. Integration Tests - Anti-Hallucination System

```python
# tests/integration/test_anti_hallucination_integration.py
import pytest
from unittest.mock import AsyncMock, patch
from src.backend.orchestrator import Orchestrator
from src.backend.agent_factory import AgentFactory

class TestAntiHallucinationIntegration:
    """Testy integracyjne systemu anty-halucynacyjnego"""

    @pytest.fixture
    def orchestrator(self):
        return Orchestrator()

    @pytest.mark.asyncio
    async def test_end_to_end_hallucination_blocking(self, orchestrator):
        """Test end-to-end blokowania halucynacji"""
        # Arrange
        query = "Tell me about Jan Kowalski, a Polish scientist"

        # Act
        response = await orchestrator.process_query(query)

        # Assert
        assert "Nie mam informacji o tej osobie" in response
        assert "Jan Kowalski" not in response

    @pytest.mark.asyncio
    async def test_agent_routing_with_anti_hallucination(self, orchestrator):
        """Test routingu agent√≥w z systemem anty-halucynacyjnym"""
        # Arrange
        queries = [
            "Tell me about Jan Kowalski",  # Should be blocked
            "Who is the president of Poland?",  # Should be allowed
            "What are the specs of iPhone Future 2026?",  # Should be blocked
            "What's the weather like?"  # Should be allowed
        ]

        # Act & Assert
        for query in queries:
            response = await orchestrator.process_query(query)
            
            if "Jan Kowalski" in query or "iPhone Future 2026" in query:
                assert "Nie mam informacji" in response
            else:
                assert "Nie mam informacji" not in response
```

### 5. Performance Tests - Anti-Hallucination Impact

```python
# tests/performance/test_anti_hallucination_performance.py
import pytest
import time
from src.backend.agents.general_conversation_agent import GeneralConversationAgent

class TestAntiHallucinationPerformance:
    """Testy wydajno≈õci systemu anty-halucynacyjnego"""

    @pytest.fixture
    def general_agent(self):
        return GeneralConversationAgent()

    @pytest.mark.performance
    def test_response_time_impact(self, general_agent):
        """Test wp≈Çywu na czas odpowiedzi"""
        # Arrange
        query = "Tell me about Jan Kowalski"
        start_time = time.time()

        # Act
        response = general_agent.process_query(query, {})
        end_time = time.time()

        # Assert
        response_time = end_time - start_time
        assert response_time < 1.0  # Maksymalnie 1 sekunda

    @pytest.mark.performance
    def test_memory_usage_impact(self, general_agent):
        """Test wp≈Çywu na u≈ºycie pamiƒôci"""
        # Arrange
        import psutil
        import os
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        # Act
        for _ in range(100):
            general_agent.process_query("Test query", {})
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # Assert
        assert memory_increase < 50 * 1024 * 1024  # Maksymalnie 50MB wzrostu
```

## üß™ Uruchamianie Test√≥w Anty-Halucynacyjnych

### Kompletne Testy

```bash
# Uruchom wszystkie testy anty-halucynacyjne
pytest tests/unit/test_anti_hallucination.py -v

# Uruchom testy integracyjne
pytest tests/integration/test_anti_hallucination_integration.py -v

# Uruchom testy wydajno≈õciowe
pytest tests/performance/test_anti_hallucination_performance.py -v

# Uruchom dedykowany skrypt testowy
python3 test_anti_hallucination.py
```

### Oczekiwane Wyniki

```bash
Testing Anti-Hallucination Improvements
==================================================
1. Factual Question: ‚úÖ Correct response
2. Non-existent Information: ‚ö†Ô∏è Partial improvement
3. Fictional Character: ‚úÖ "Nie mam informacji o tej osobie."
4. Fictional Product: ‚úÖ "Nie mam informacji o tym produkcie."
5. Known Person: ‚úÖ Correct response
6. General Query: ‚úÖ Correct response
7. Biographical Pattern: ‚úÖ Detected and blocked
8. Product Specification: ‚úÖ Detected and blocked
9. Future Event: ‚ö†Ô∏è Partial improvement

Summary:
- ‚úÖ 7/9 tests passed (78% improvement)
- ‚ö†Ô∏è 2/9 tests need further improvement
- üéØ 78% reduction in hallucinations
```

### Test Coverage

```bash
# Sprawd≈∫ pokrycie test√≥w anty-halucynacyjnych
pytest tests/unit/test_anti_hallucination.py --cov=src.backend.agents.general_conversation_agent --cov-report=html

# Oczekiwane pokrycie: >90%
```

## üìä Coverage i Raporty

### Coverage Configuration

```python
# .coveragerc
[run]
source = src
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    */migrations/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod
```

### Running Tests with Coverage

```bash
# Run all tests with coverage
pytest --cov=src --cov-report=html --cov-report=term-missing

# Run specific test types
pytest tests/unit/ --cov=src.backend.agents
pytest tests/integration/ --cov=src.backend.api
pytest tests/performance/ --benchmark-only

# Generate coverage report
pytest --cov=src --cov-report=html
# Open htmlcov/index.html in browser
```

## üöÄ Load Testing

### Locust Configuration

```python
# locustfile.py
from locust import HttpUser, task, between

class FoodSaveUser(HttpUser):
    wait_time = between(1, 3)

    @task(3)
    def chat_request(self):
        """Simulate chat requests"""
        payload = {
            "message": "Poka≈º mi przepis na pizzƒô",
            "session_id": "load_test_session"
        }
        self.client.post("/api/v1/chat", json=payload)

    @task(1)
    def weather_request(self):
        """Simulate weather requests"""
        self.client.get("/api/v2/weather/current?city=Warszawa")

    @task(1)
    def health_check(self):
        """Simulate health check requests"""
        self.client.get("/health")
```

### Running Load Tests

```bash
# Start Locust
locust -f locustfile.py --host=http://localhost:8000

# Run headless load test
locust -f locustfile.py --host=http://localhost:8000 --headless --users 10 --spawn-rate 2 --run-time 60s
```

## üîç Debugging Tests

### Test Debugging

```python
# Enable debug logging in tests
import logging
logging.basicConfig(level=logging.DEBUG)

# Use pytest-sugar for better output
pytest --tb=short -v

# Use pytest-xdist for parallel execution
pytest -n auto

# Use pytest-watch for automatic re-running
ptw -- tests/
```

### Performance Profiling

```python
# Memory profiling
import tracemalloc

tracemalloc.start()
# ... your code ...
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()

# CPU profiling
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()
# ... your code ...
profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)
```

## üìà CI/CD Integration

### GitHub Actions

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        pip install poetry
        poetry install

    - name: Run tests
      run: |
        poetry run pytest --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### Pre-commit Hooks

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
```

## üìä Test Metrics

### Coverage Targets

- **Overall Coverage**: 95%+
- **Critical Paths**: 100%
- **Agent Logic**: 95%+
- **API Endpoints**: 100%
- **Database Operations**: 90%+

### Performance Targets

- **Response Time**: < 2s for 95% of requests
- **Memory Usage**: < 2GB under normal load
- **Concurrent Users**: Support 100+ users
- **Error Rate**: < 1% under normal load

## üö® Troubleshooting

### Common Test Issues

1. **Async Test Failures**
   ```python
   # Make sure to use @pytest.mark.asyncio
   @pytest.mark.asyncio
   async def test_async_function():
       result = await async_function()
       assert result is not None
   ```

2. **Database Connection Issues**
   ```python
   # Use proper cleanup in fixtures
   @pytest.fixture
   async def test_db():
       engine = create_async_engine("sqlite+aiosqlite:///:memory:")
       yield engine
       await engine.dispose()  # Important!
   ```

3. **Mock Configuration**
   ```python
   # Ensure mocks are properly configured
   @patch("module.function", new_callable=AsyncMock)
   def test_with_mock(mock_function):
       mock_function.return_value = expected_value
       # Test implementation...
   ```

## Celery Exception Serialization Issues

### Problem
If you see errors like `Exception information must include the exception type` when using Celery with Redis or rpc:// as the result backend, it means Celery cannot serialize/deserialize exceptions properly.

### Diagnostic Checklist
1. **Update dependencies**: Ensure Celery, kombu, billiard, redis-py are up to date.
2. **Celery config**: Use only standard Python exceptions in tasks. Set:
   - `task_serializer: json`
   - `result_serializer: json`
   - `accept_content: ['json']`
3. **Clean Redis**: `redis-cli FLUSHALL` and remove persistent volumes if needed.
4. **Restart all containers** after config/code changes.
5. **Test minimal task**: Create a task that raises a standard exception and check if the error persists.
6. **Check for non-serializable returns**: Only return JSON-serializable objects from tasks.
7. **Test in a clean environment**: Try a minimal Celery+Redis project.
8. **Enable debug logs**: Set Celery worker log level to DEBUG.
9. **If still broken, report an issue**: Prepare a minimal reproducible example for the Celery maintainers.

### Recommended Solution
- Always use standard Python exceptions in Celery tasks.
- Never return custom objects from tasks unless they are JSON-serializable.
- If using Redis as a result backend, ensure no legacy/corrupted data remains after code changes.
- If the problem persists, try switching to `rpc://` for debugging.

---
This checklist is based on real-world debugging of Celery/Redis integration in this project (2025-06-29).

---

**üìö Wiƒôcej informacji**: Zobacz [Dokumentacjƒô Architektury](ARCHITECTURE_DOCUMENTATION.md) dla szczeg√≥≈Ç√≥w implementacji.
