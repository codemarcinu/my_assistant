# ğŸ§  Zaawansowane ZarzÄ…dzanie Kontekstem Konwersacji

## ğŸ“‹ PrzeglÄ…d

System rozwiÄ…zuje problem gubienia wÄ…tku w chatbotach AI poprzez:

- **TrwaÅ‚e przechowywanie** konwersacji w bazie danych PostgreSQL
- **InteligentnÄ… kompresjÄ™** dÅ‚ugich rozmÃ³w (sliding window + podsumowania)
- **OptymalizacjÄ™ okna kontekstowego** dla LLM
- **Semantyczny cache** podobnych kontekstÃ³w
- **Automatyczne zarzÄ…dzanie pamiÄ™ciÄ…** z cleanup
- **SzczegÃ³Å‚owe statystyki i monitorowanie**

## ğŸ—ï¸ Architektura

### Komponenty systemu

```mermaid
graph TB
    A[MemoryManager] --> B[MemoryContext]
    A --> C[ConversationSummary]
    A --> D[ContextWindow]
    A --> E[PersistentStorage]
    A --> F[SemanticCache]
    
    B --> G[History]
    B --> H[ActiveAgents]
    B --> I[Optimization]
    
    C --> J[KeyPoints]
    C --> K[Topics]
    C --> L[Preferences]
    
    D --> M[RecentMessages]
    D --> N[Summary]
    D --> O[TokenCount]
```

### Kluczowe klasy

#### MemoryContext
```python
class MemoryContext:
    """Rozszerzony kontekst pamiÄ™ci z trwaÅ‚ym przechowywaniem"""
    
    def get_optimized_context(self, max_tokens: int = 4000) -> List[Dict[str, Any]]:
        """Zwraca zoptymalizowany kontekst dla LLM w limicie tokenÃ³w"""
    
    def _optimize_context_window(self, max_tokens: int) -> None:
        """Optimize context window to fit within token limit"""
    
    def _create_conversation_summary(self, messages: List[Dict]) -> ConversationSummary:
        """Create conversation summary from older messages"""
```

#### ConversationSummary
```python
class ConversationSummary:
    """Podsumowanie konwersacji dla kompresji kontekstu"""
    
    key_points: List[str]  # Kluczowe punkty (preferencje, pytania, odpowiedzi)
    topics_discussed: List[str]  # Tematy rozmowy (PL/EN)
    user_preferences: Dict[str, Any]  # Preferencje uÅ¼ytkownika
    conversation_style: str  # Styl rozmowy (friendly, formal, casual)
    
    def format_for_llm(self) -> str:
        """Format podsumowania do kontekstu LLM"""
```

#### ContextWindow
```python
class ContextWindow:
    """Okno kontekstowe z optymalizacjÄ…"""
    
    def get_optimized_messages(self) -> List[Dict[str, Any]]:
        """Zwraca zoptymalizowane wiadomoÅ›ci dla LLM"""
```

## ğŸ”§ Implementacja

### 1. TrwaÅ‚e przechowywanie
- Konwersacje i wiadomoÅ›ci zapisywane w PostgreSQL
- Automatyczne odtwarzanie kontekstu po restarcie

### 2. Inteligentna kompresja i podsumowania
- Ostatnie 10 wiadomoÅ›ci w peÅ‚nej formie
- Starsze wiadomoÅ›ci podsumowywane:
    - **WielojÄ™zyczne wykrywanie tematÃ³w** (PL/EN)
    - **Ekstrakcja preferencji uÅ¼ytkownika** (likes, dislikes, wants, has)
    - **Wydobywanie kluczowych pytaÅ„ i odpowiedzi**
    - **Styl rozmowy** (friendly, formal, casual)
    - **Top 5 kluczowych punktÃ³w**

### 3. Semantyczny cache
- Szybkie wyszukiwanie podobnych kontekstÃ³w po hashach

### 4. Automatyczne zarzÄ…dzanie pamiÄ™ciÄ…
- Weak references, cleanup, limity, statystyki

### 5. Statystyki i monitorowanie
- Kompresja, cache hit rate, liczba kontekstÃ³w, wiek najstarszego/najnowszego

## ğŸ“Š API Endpoints

### Pobieranie historii z optymalizacjÄ…
```http
GET /api/chat/memory_chat?limit=50
```

Response:
```json
{
  "success": true,
  "data": [...],
  "memory_stats": {
    "compression_ratio": 0.3,
    "persistent_contexts": 15,
    "cached_contexts": 8
  }
}
```

### Statystyki pamiÄ™ci
```http
GET /api/chat/memory_stats
```

Response:
```json
{
  "success": true,
  "stats": {
    "total_contexts": 25,
    "persistent_contexts": 15,
    "cached_contexts": 8,
    "compression_ratio": 0.3,
    "cache_hit_rate": 0.75,
    "max_contexts": 1000,
    "cleanup_threshold": 800,
    "oldest_context": "2024-06-01T12:00:00Z",
    "newest_context": "2024-06-02T10:00:00Z"
  }
}
```

### Optymalizacja pamiÄ™ci
```http
POST /api/chat/memory_optimize?session_id=optional
```

## ğŸ¯ KorzyÅ›ci

- **Brak gubienia wÄ…tku**: kontekst zachowany nawet przy dÅ‚ugich rozmowach
- **WydajnoÅ›Ä‡ i skalowalnoÅ›Ä‡**: automatyczne czyszczenie, cache, weak references
- **Monitorowanie**: szczegÃ³Å‚owe statystyki, API, frontend
- **ÅatwoÅ›Ä‡ rozbudowy**: prosta integracja z agentami i API

## ğŸ“ˆ RozwÃ³j

- MoÅ¼liwoÅ›Ä‡ podpiÄ™cia LLM do automatycznego podsumowywania
- Rozszerzenie wykrywania preferencji i tematÃ³w
- Integracja z innymi ÅºrÃ³dÅ‚ami wiedzy (RAG, profile)

## ğŸ” Monitorowanie

### Frontend - MemoryMonitorModule

Komponent React do monitorowania stanu pamiÄ™ci:

```typescript
export const MemoryMonitorModule: React.FC = () => {
  const [stats, setStats] = useState<MemoryStats | null>(null);
  
  const fetchStats = async () => {
    const response = await chatAPI.getMemoryStats();
    if (response.data && response.data.stats) {
      setStats(response.data.stats);
    }
  };
  
  // WyÅ›wietla:
  // - UÅ¼ycie pamiÄ™ci (progress bar)
  // - RozkÅ‚ad kontekstÃ³w (aktywne/persystentne/cache)
  // - Metryki wydajnoÅ›ci (kompresja, cache hit rate)
  // - Przycisk optymalizacji
};
```

### Metryki wydajnoÅ›ci

1. **Compression Ratio**: Stosunek skompresowanych do oryginalnych wiadomoÅ›ci
2. **Cache Hit Rate**: Procent trafieÅ„ w cache semantyczny
3. **Memory Usage**: Wykorzystanie dostÄ™pnej pamiÄ™ci
4. **Context Age**: Wiek najstarszych i najnowszych kontekstÃ³w

## ğŸš€ UÅ¼ycie

### W agentach konwersacyjnych

```python
# W GeneralConversationAgent
def _prepare_messages(self, query: str, conversation_history: List[Dict[str, str]], context: str):
    # UÅ¼yj zoptymalizowanej historii jeÅ›li dostÄ™pna
    if hasattr(conversation_history, 'get_optimized_context'):
        optimized_messages = conversation_history.get_optimized_context(max_tokens=3000)
        messages.extend(optimized_messages)
    else:
        # Fallback: ostatnie 15 wiadomoÅ›ci
        max_history_messages = 15
        recent_history = conversation_history[-max_history_messages:]
        for entry in recent_history:
            messages.append({"role": entry["role"], "content": entry["content"]})
```

### W API endpoints

```python
# W chat endpoint
context = await orchestrator.memory_manager.get_context(request.session_id)
context.add_message(role="user", content=request.message)
context.add_message(role="assistant", content=response.text)
await orchestrator.memory_manager.update_context(context)
```

## ğŸ”§ Konfiguracja

### Parametry MemoryManager

```python
MemoryManager(
    max_contexts=1000,
    cleanup_threshold_ratio=0.8,
    enable_persistence=True,
    enable_semantic_cache=True
)
```

## ğŸ“ PrzykÅ‚ad podsumowania konwersacji

```json
{
  "key_points": [
    "User likes: i like spaghetti with tomato sauce",
    "User asked: what ingredients do i need?",
    "Assistant provided: here's a simple recipe for spaghetti with tomato sauce..."
  ],
  "topics_discussed": ["cooking", "technology"],
  "user_preferences": {},
  "conversation_style": "friendly"
}
```

## ğŸ“ˆ Metryki i monitoring

### Prometheus metrics

```python
# Memory usage
memory_contexts_total = Gauge('memory_contexts_total', 'Total number of contexts')
memory_persistent_contexts = Gauge('memory_persistent_contexts', 'Persistent contexts')
memory_compression_ratio = Gauge('memory_compression_ratio', 'Compression efficiency')

# Performance
memory_cache_hit_rate = Gauge('memory_cache_hit_rate', 'Cache hit rate')
memory_cleanup_count = Counter('memory_cleanup_count', 'Number of cleanups')
```

### Grafana dashboard

Dashboard do monitorowania:
- UÅ¼ycie pamiÄ™ci w czasie rzeczywistym
- EfektywnoÅ›Ä‡ kompresji
- Cache hit rate
- Liczba kontekstÃ³w w rÃ³Å¼nych warstwach

## ğŸ”® RozwÃ³j przyszÅ‚oÅ›ci

### Planowane ulepszenia

1. **Zaawansowana analiza semantyczna**
   - Embedding-based similarity dla cache'u
   - NLP-based topic extraction

2. **Adaptacyjna kompresja**
   - Dynamiczne dostosowywanie strategii kompresji
   - Machine learning dla optymalizacji

3. **Distributed memory**
   - Redis cluster dla skalowania
   - Replikacja kontekstÃ³w

4. **Advanced monitoring**
   - Real-time alerts
   - Predictive maintenance
   - A/B testing rÃ³Å¼nych strategii

## ğŸ“š Podsumowanie

Implementacja zaawansowanego zarzÄ…dzania kontekstem konwersacji rozwiÄ…zuje fundamentalny problem chatbotÃ³w AI - gubienie wÄ…tku. System zapewnia:

âœ… **TrwaÅ‚oÅ›Ä‡ konwersacji** - kontekst zachowany miÄ™dzy sesjami  
âœ… **InteligentnÄ… kompresjÄ™** - optymalne wykorzystanie okna kontekstowego  
âœ… **WydajnoÅ›Ä‡** - cache semantyczny i automatyczne zarzÄ…dzanie pamiÄ™ciÄ…  
âœ… **Monitorowanie** - szczegÃ³Å‚owe metryki i diagnostyka  
âœ… **SkalowalnoÅ›Ä‡** - obsÅ‚uga tysiÄ™cy rÃ³wnoczesnych konwersacji  

System jest gotowy do produkcji i moÅ¼e byÄ‡ dalej rozwijany zgodnie z potrzebami aplikacji. 